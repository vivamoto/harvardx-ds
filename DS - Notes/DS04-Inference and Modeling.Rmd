---
title: "DS04 - Inference and Modeling"
author: "Victor Ivamoto"
date: "02/11/2019"
output: 
  html_document: 
    toc: true
    toc_float: true
    number_sections: true
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Section 1: Parameters and Estimates

Section 1 introduces you to parameters and estimates.

After completing Section 1, you will be able to:

- Understand how to use a sampling model to perform a poll.
- Explain the terms **population**, **parameter**, and **sample** as they relate to statistical inference.
- Use a sample to estimate the population proportion from the sample average.
- Calculate the expected value and standard error of the sample average. 

There is 1 assignment that uses the DataCamp platform for you to practice your coding skills.

We encourage you to use R to interactively test out your answers and further your learning.


## Sampling Model Parameters and Estimates

**Textbook link**  
This video matches the textbook sections on [the sampling model for polls](https://rafalab.github.io/dsbook/inference.html#the-sampling-model-for-polls) and the first part of [populations, samples, parameters and estimates.](https://rafalab.github.io/dsbook/inference.html#populations-samples-parameters-and-estimates)

**Key points**  

- The task of statistical inference is to estimate an unknown population parameter using observed data from a sample.
- In a sampling model, the collection of elements in the urn is called the *population*.
- A *parameter* is a number that summarizes data for an entire population.
- A *sample* is observed data from a subset of the population.
- An *estimate* is a summary of the observed data about a parameter that we believe is informative. It is a data-driven guess of the population parameter.
- We want to predict the proportion of the blue beads in the urn, the parameter $p$. The proportion of red beads in the urn is $1−p$ and the *spread* is $2p−1$.
- The sample proportion is a random variable. Sampling gives random results drawn from the population distribution.

**Code: Function for taking a random draw from a specific urn**  
The `dslabs` package includes a function for taking a random draw of size $n$ from the urn described in the video:

```{r}
library(tidyverse)
library(dslabs)
take_poll(25)    # draw 25 beads
```

## The Sample Average
**Textbook links**  
This video matches the [textbook section on the sample average](https://rafalab.github.io/dsbook/inference.html#the-sample-average) and the [textbook section on parameters.](https://rafalab.github.io/dsbook/inference.html#parameters)

**Key points**  

- Many common data science tasks can be framed as estimating a parameter from a sample.
- We illustrate statistical inference by walking through the process to estimate $p$. From the estimate of $p$, we can easily calculate an estimate of the spread, $2p−1$.
- Consider the random variable $X$ that is 1 if a blue bead is chosen and 0 if a red bead is chosen. The proportion of blue beads in $N$ draws is the average of the draws $X_1,...,X_N$.
- $\bar{X}$  is the *sample average*. In statistics, a bar on top of a symbol denotes the average. $\bar{X}$ is a random variable because it is the average of random draws - each time we take a sample, $\bar{X}$ is different.
\[\bar{X}=\frac{X_1+X_2+...+X_N}{N}\]
- The number of blue beads drawn in $N$ draws, $N\bar{X}$, is $N$ times the proportion of values in the urn. However, we do not know the true proportion: we are trying to estimate this parameter $p$.

## Polling versus Forecasting
**Textbook link**  
This video corresponds to the [textbook section on polling versus forecasting.](https://rafalab.github.io/dsbook/inference.html#polling-versus-forecasting)

**Key points**  

- A poll taken in advance of an election estimates $p$ for that moment, not for election day.
- In order to predict election results, forecasters try to use early estimates of $p$ to predict $p$ on election day. We discuss some approaches in later sections.

## Properties of Our Estimate
**Clarification**  
At 2:47, the audio states that the SE is "0.15, or 1.5 percentage points". The audio should state that the SE is 0.015. The percentage shown on the slide is correct, and the transcript has been corrected.

**Textbook link**  
This video corresponds to the [textbook section on properties of our estimate.](https://rafalab.github.io/dsbook/inference.html#properties-of-our-estimate-expected-value-and-standard-error)

**Key points**  

- When interpreting values of $\bar{X}$, it is important to remember that $\bar{X}$ is a random variable with an expected value and standard error that represents the sample proportion of positive events.
- The expected value of $\bar{X}$ is the parameter of interest $p$. This follows from the fact that $\bar{X}$ is the sum of independent draws of a random variable times a constant $1/N$.
\[E(\bar{X})=p \]
- As the number of draws $N$ increases, the standard error of our estimate $\bar{X}$ decreases. The standard error of the average of $\bar{X}$ over $N$ draws is:
\[SE(\bar{X})=\sqrt{p(1−p)/N}\]
- In theory, we can get more accurate estimates of $p$ by increasing $N$. In practice, there are limits on the size of $N$ due to costs, as well as other factors we discuss later.
- We can also use other random variable equations to determine the expected value of the sum of draws $E(S)$ and standard error of the sum of draws $SE(S)$.
\[E(S)=Np\]
\[SE(S)=\sqrt{Np(1−p)}\]

# Section 2: The Central Limit Theorem in Practice
In Section 2, you will look at the Central Limit Theorem in practice.

After completing Section 2, you will be able to:

- Use the Central Limit Theorem to calculate the probability that a sample estimate $\bar{X}$ is close to the population proportion $p$.
- Run a Monte Carlo simulation to corroborate theoretical results built using probability theory.
- Estimate the spread based on estimates of $\bar{X}$ and $\hat{SE}(\bar{X})$ .
- Understand why bias can mean that larger sample sizes aren't necessarily better.

There is 1 assignment that uses the DataCamp platform for you to practice your coding skills.

We encourage you to use R to interactively test out your answers and further your learning.

## The Central Limit Theorem in Practice
**Textbook link**  

This video corresponds to the [textbook section on the Central Limit Theorem in practice.](https://rafalab.github.io/dsbook/inference.html#clt)

**Key points**  

- Because $\bar{X}$ is the sum of random draws divided by a constant, the distribution of $\bar{X}$ is approximately normal.
- We can convert $\bar{X}$ to a standard normal random variable $Z$: 
\[Z=\frac{\bar{x}−E(\bar{x})}{SE(\bar{x})}\]
- The probability that $\bar{X}$ is within .01 of the actual value of $p$ is:
\[Pr(Z≤.01/\sqrt{p(1−p)/N})−Pr(Z≤−.01/\sqrt{p(1−p)/N})\]
- The Central Limit Theorem (CLT) still works if $\bar{X}$ is used in place of $p$. This is called a *plug-in estimate*. Hats over values denote estimates. Therefore:
\[\hat{SE}(\bar{X})=\sqrt{\bar{X}(1−\bar{X})/N}\]
- Using the CLT, the probability that $\bar{X}$ is within .01 of the actual value of $p$ is:
\[Pr(Z≤.01/\sqrt{\bar{X}(1−\bar{X})/N})−Pr(Z≤−.01/\sqrt{\bar{X}(1−\bar{X})/N)}\]

**Code: Computing the probability of** $\mathbf{\bar{X}}$ **being within .01 of *p***

```{r}
X_hat <- 0.48
se <- sqrt(X_hat*(1-X_hat)/25)
pnorm(0.01/se) - pnorm(-0.01/se)
```

## Margin of Error
**Textbook link**  

The margin of error is discussed within the [textbook section on the Central Limit Theorem in practice.](https://rafalab.github.io/dsbook/inference.html#clt)

**Key points**  

- The *margin of error* is defined as 2 times the standard error of the estimate $\bar{X}$ .
- There is about a 95% chance that $\bar{X}$ will be within two standard errors of the actual parameter $p$.

## A Monte Carlo Simulation for the CLT
**Textbook link**  
This video corresponds to the [textbook section on a Monte Carlo simulation for the CLT.](https://rafalab.github.io/dsbook/inference.html#a-monte-carlo-simulation)

**Key points**  

- We can run Monte Carlo simulations to compare with theoretical results assuming a value of $p$.
- In practice, $p$ is unknown. We can corroborate theoretical results by running Monte Carlo simulations with one or several values of $p$.
- One practical choice for $p$ when modeling is $\bar{X}$, the observed value of $\hat{X}$ in a sample.

**Code: Monte Carlo simulation using a set value of *p***
```{r}
p <- 0.45    # unknown p to estimate
N <- 1000

# simulate one poll of size N and determine x_hat
x <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
x_hat <- mean(x)

# simulate B polls of size N and determine average x_hat
B <- 10000    # number of replicates
N <- 1000    # sample size per replicate
x_hat <- replicate(B, {
    x <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
    mean(x)
})
```

**Code: Histogram and QQ-plot of Monte Carlo results**
```{r}
library(tidyverse)

library(gridExtra)
p1 <- data.frame(x_hat = x_hat) %>%
    ggplot(aes(x_hat)) +
    geom_histogram(binwidth = 0.005, color = "black")
p2 <- data.frame(x_hat = x_hat) %>%
    ggplot(aes(sample = x_hat)) +
    stat_qq(dparams = list(mean = mean(x_hat), sd = sd(x_hat))) +
    geom_abline() +
    ylab("X_hat") +
    xlab("Theoretical normal")
grid.arrange(p1, p2, nrow=1)
```

## The Spread
**Textbook link**  
This video corresponds to the [textbook section on the spread.](https://rafalab.github.io/dsbook/inference.html#the-spread)

**Key points**  

- The spread between two outcomes with probabilities $p$ and $1−p$ is $2p−1$.
- The expected value of the spread is $2\bar{X}−1$.
- The standard error of the spread is $2\hat{SE}(\bar{X})$.
- The margin of error of the spread is 2 times the margin of error of $\bar{X}$.

## Bias: Why Not Run a Very Large Poll?
**Textbook link**  

This video corresponds to the [textbook section on bias.](https://rafalab.github.io/dsbook/inference.html#bias-why-not-run-a-very-large-poll)

**Key points**  

- An extremely large poll would theoretically be able to predict election results almost perfectly.
- These sample sizes are not practical. In addition to cost concerns, polling doesn't reach everyone in the population (eventual voters) with equal probability, and it also may include data from outside our population (people who will not end up voting).
- These systematic errors in polling are called *bias*. We will learn more about bias in the future.

**Code: Plotting margin of error in an extremely large poll over a range of values of *p***
```{r}
library(tidyverse)
N <- 100000
p <- seq(0.35, 0.65, length = 100)
SE <- sapply(p, function(x) 2*sqrt(x*(1-x)/N))
data.frame(p = p, SE = SE) %>%
    ggplot(aes(p, SE)) +
    geom_line()
```

# Section 3: Confidence Intervals and p-Values 
In Section 3, you will look at confidence intervals and p-values.

After completing Section 3, you will be able to:

- Calculate confidence intervals of difference sizes around an estimate.
- Understand that a confidence interval is a random interval with the given probability of falling on top of the parameter.
- Explain the concept of "power" as it relates to inference.
- Understand the relationship between p-values and confidence intervals and explain why reporting confidence intervals is often preferable.

There is 1 assignment that uses the DataCamp platform for you to practice your coding skills.

We encourage you to use R to interactively test out your answers and further your learning.

## Confidence Intervals
**Corrections**
There are two minor errors in the video:

At 4:28 the formula should be `pnorm(qnorm(1-0.995))` instead of `pnorm(1-qnorm(0.995))`. This has been corrected in the code below.
At 4:50, the equation should be $1−\frac{1−q}{2}−\frac{1−q}{2}=1−(1−q)=q$.

**Textbook link**  
This video corresponds to the first part of the [textbook section on confidence intervals.](https://rafalab.github.io/dsbook/inference.html#confidence-intervals)

**Key points**  

- We can use statistical theory to compute the probability that a given interval contains the true parameter $p$.
- 95% confidence intervals are intervals constructed to have a 95% chance of including $p$. The margin of error is approximately a 95% confidence interval.
- The start and end of these confidence intervals are random variables.
- To calculate any size confidence interval, we need to calculate the value $z$ for which $Pr(−z≤Z≤z)$ equals the desired confidence. For example, a 99% confidence interval requires calculating $z$ for $Pr(−z≤Z≤z)=0.99$.
- For a confidence interval of size $q$, we solve for $z=1−\frac{1−q}2$.
- To determine a 95% confidence interval, use `z <- qnorm(0.975)`. This value is slightly smaller than 2 times the standard error.

**Code: geom_smooth confidence interval example**  
The shaded area around the curve is related to the concept of confidence intervals.

```{r}
data("nhtemp")
data.frame(year = as.numeric(time(nhtemp)), temperature = as.numeric(nhtemp)) %>%
    ggplot(aes(year, temperature)) +
    geom_point() +
    geom_smooth() +
    ggtitle("Average Yearly Temperatures in New Haven")
```

**Code: Monte Carlo simulation of confidence intervals**  
Note that to compute the exact 95% confidence interval, we would use `qnorm(.975)*SE_hat` instead of `2*SE_hat`.

```{r}
p <- 0.45
N <- 1000
X <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))    # generate N observations
X_hat <- mean(X)    # calculate X_hat
SE_hat <- sqrt(X_hat*(1-X_hat)/N)    # calculate SE_hat, SE of the mean of N observations
c(X_hat - 2*SE_hat, X_hat + 2*SE_hat)    # build interval of 2*SE above and below mean
```

**Code: Solving for $z$ with `qnorm`**  
```{r}
z <- qnorm(0.995)    # calculate z to solve for 99% confidence interval
pnorm(qnorm(0.995))    # demonstrating that qnorm gives the z value for a given probability
pnorm(qnorm(1-0.995))    # demonstrating symmetry of 1-qnorm
pnorm(z) - pnorm(-z)    # demonstrating that this z value gives correct probability for interval
```

## A Monte Carlo Simulation for Confidence Intervals
**Textbook link**  
This video corresponds to the [textbook section on a Monte Carlo simulation for confidence intervals.](https://rafalab.github.io/dsbook/inference.html#a-monte-carlo-simulation-1)

**Key points**  

- We can run a Monte Carlo simulation to confirm that a 95% confidence interval contains the true value of $p$ 95% of the time.
- A plot of confidence intervals from this simulation demonstrates that most intervals include $p$, but roughly 5% of intervals miss the true value of $p$.

**Code: Monte Carlo simulation**  
Note that to compute the exact 95% confidence interval, we would use `qnorm(.975)*SE_hat` instead of `2*SE_hat`.

```{r}
B <- 10000
inside <- replicate(B, {
    X <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
    X_hat <- mean(X)
    SE_hat <- sqrt(X_hat*(1-X_hat)/N)
    between(p, X_hat - 2*SE_hat, X_hat + 2*SE_hat)    # TRUE if p in confidence interval
})
mean(inside)
```

## The Correct Language
**Textbook link**  
This video corresponds to the [textbook section on the correct language for confidence intervals.](https://rafalab.github.io/dsbook/inference.html#the-correct-language)

**Key points**  

- The 95% confidence intervals are random, but $p$ is not random.
- 95% refers to the probability that the random interval falls on top of $p$.
- It is technically incorrect to state that $p$ has a 95% chance of being in between two values because that implies $p$ is random.

## Power
**Textbook link**  
This video corresponds to the [textbook section on power.](https://rafalab.github.io/dsbook/inference.html#power)

**Key points**  

- If we are trying to predict the result of an election, then a confidence interval that includes a spread
of 0 (a tie) is not helpful.
- A confidence interval that includes a spread of 0 does not imply a close election, it means the sample size is too small.
- Power is the probability of detecting an effect when there is a true effect to find. Power increases as sample size increases, because larger sample size means smaller standard error.

**Code: Confidence interval for the spread with sample size of 25**  
Note that to compute the exact 95% confidence interval, we would use `c(-qnorm(.975), qnorm(.975))` instead of 1.96.

```{r}
N <- 25
X_hat <- 0.48
(2*X_hat - 1) + c(-2, 2)*2*sqrt(X_hat*(1-X_hat)/N)
```

## p-Values
**Textbook link**  
This video corresponds to the [textbook section on p-values.](https://rafalab.github.io/dsbook/inference.html#p-values)

**Key points**  

- The null hypothesis is the hypothesis that there is no effect. In this case, the null hypothesis is that the spread is 0, or $p=0.5$.
- The p-value is the probability of detecting an effect of a certain size or larger when the null hypothesis is true.
- We can convert the probability of seeing an observed value under the null hypothesis into a standard normal random variable. We compute the value of $z$ that corresponds to the observed result, and then use that $z$ to compute the p-value.
- If a 95% confidence interval does not include our observed value, then the p-value must be smaller than 0.05.
- It is preferable to report confidence intervals instead of p-values, as confidence intervals give information about the size of the estimate and p-values do not.

**Code: Computing a p-value for observed spread of 0.02**

```{r}
N <- 100    # sample size
z <- sqrt(N) * 0.02/0.5    # spread of 0.02
1 - (pnorm(z) - pnorm(-z))
```

# Section 4: Statistical Models
In Section 4, you will look at statistical models in the context of election polling and forecasting.

After completing Section 4, you will be able to:

- Understand how aggregating data from different sources, as poll aggregators do for poll data, can improve the precision of a prediction.
- Understand how to fit a multilevel model to the data to forecast, for example, election results.
- Explain why a simple aggregation of data is insufficient to combine results because of factors such as pollster bias.
- Use a data-driven model to account for additional types of sampling variability such as pollster-to-pollster variability.

There is 1 assignment that uses the DataCamp platform for you to practice your coding skills.

We encourage you to use R to interactively test out your answers and further your learning.

## Poll Aggregators
**Textbook link**  
This video corresponds to the [textbook chapter introduction for statistical models](https://rafalab.github.io/dsbook/models.html) and the [textbook section on poll aggregators.](https://rafalab.github.io/dsbook/models.html#poll-aggregators)

**Key points**  

- Poll aggregators combine the results of many polls to simulate polls with a large sample size and therefore generate more precise estimates than individual polls.
- Polls can be simulated with a Monte Carlo simulation and used to construct an estimate of the spread and confidence intervals.
- The actual data science exercise of forecasting elections involves more complex statistical modeling, but these underlying ideas still apply.

**Code: Simulating polls**  
Note that to compute the exact 95% confidence interval, we would use `qnorm(.975)*SE_hat` instead of `2*SE_hat`.

```{r}
d <- 0.039
Ns <- c(1298, 533, 1342, 897, 774, 254, 812, 324, 1291, 1056, 2172, 516)
p <- (d+1)/2

# calculate confidence intervals of the spread
confidence_intervals <- sapply(Ns, function(N){
    X <- sample(c(0,1), size=N, replace=TRUE, prob = c(1-p, p))
    X_hat <- mean(X)
    SE_hat <- sqrt(X_hat*(1-X_hat)/N)
    2*c(X_hat, X_hat - 2*SE_hat, X_hat + 2*SE_hat) - 1
})

# generate a data frame storing results
polls <- data.frame(poll = 1:ncol(confidence_intervals),
                    t(confidence_intervals), sample_size = Ns)
names(polls) <- c("poll", "estimate", "low", "high", "sample_size")
polls
```

**Code: Calculating the spread of combined polls**  
Note that to compute the exact 95% confidence interval, we would use `qnorm(.975)` instead of 1.96.

```{r}
d_hat <- polls %>%
    summarize(avg = sum(estimate*sample_size) / sum(sample_size)) %>%
    .$avg

p_hat <- (1+d_hat)/2
moe <- 2*1.96*sqrt(p_hat*(1-p_hat)/sum(polls$sample_size))   
round(d_hat*100,1)
round(moe*100, 1)
```

## Pollsters and Multilevel Models
**Textbook link**  
This video corresponds to material within the [textbook section on poll aggregators.](https://rafalab.github.io/dsbook/models.html#poll-aggregators)

**Key points**  

- Different poll aggregators generate different models of election results from the same poll data. This is because they use different statistical models.
- We will use actual polling data about the popular vote from the 2016 US presidential election to learn the principles of statistical modeling.

## Poll Data and Pollster Bias
**Textbook links**  
This section corresponds to the [textbook section on poll data](https://rafalab.github.io/dsbook/models.html#poll-data) and the [textbook section on pollster bias.](https://rafalab.github.io/dsbook/models.html#pollster-bias)

**Key points**  

- We analyze real 2016 US polling data organized by FiveThirtyEight. We start by using reliable national polls taken within the week before the election to generate an urn model.
- Consider $p$ the proportion voting for Clinton and $1−p$ the proportion voting for Trump. We are interested in the spread $d=2p−1$.
- Poll results are a random normal variable with expected value of the spread $d$ and standard error  $2\sqrt{p(1−p)/N}$.
- Our initial estimate of the spread did not include the actual spread. Part of the reason is that different pollsters have different numbers of polls in our dataset, and each pollster has a bias.
- *Pollster bias* reflects the fact that repeated polls by a given pollster have an expected value different from the actual spread and different from other pollsters. Each pollster has a different bias.
- The urn model does not account for pollster bias. We will develop a more flexible data-driven model that can account for effects like bias.

**Code: Generating simulated poll data**  
```{r}
library(dslabs)
data(polls_us_election_2016)
names(polls_us_election_2016)

# keep only national polls from week before election with a grade considered reliable
polls <- polls_us_election_2016 %>%
    filter(state == "U.S." & enddate >= "2016-10-31" &
               (grade %in% c("A+", "A", "A-", "B+") | is.na(grade)))

# add spread estimate
polls <- polls %>%
    mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)

# compute estimated spread for combined polls
d_hat <- polls %>%
    summarize(d_hat = sum(spread * samplesize) / sum(samplesize)) %>%
    .$d_hat

# compute margin of error
p_hat <- (d_hat+1)/2
moe <- 1.96 * 2 * sqrt(p_hat*(1-p_hat)/sum(polls$samplesize))

# histogram of the spread
polls %>%
    ggplot(aes(spread)) +
    geom_histogram(color="black", binwidth = .01)
```

**Code: Investigating poll data and pollster bias**  
```{r}
# number of polls per pollster in week before election
polls %>% group_by(pollster) %>% summarize(n())
# plot results by pollsters with at least 6 polls
polls %>% group_by(pollster) %>%
    filter(n() >= 6) %>%
    ggplot(aes(pollster, spread)) +
    geom_point() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
# standard errors within each pollster
polls %>% group_by(pollster) %>%
    filter(n() >= 6) %>%
    summarize(se = 2 * sqrt(p_hat * (1-p_hat) / median(samplesize)))
```

## Data-Driven Models
**Textbook links**  
This video corresponds to the textbook section on data-driven models.

**Key points**  

- Instead of using an urn model where each poll is a random draw from the same distribution of voters, we instead define a model using an urn that contains poll results from all possible pollsters.
- We assume the expected value of this model is the actual spread $d=2p−1$.
- Our new standard error $\sigma$ now factors in pollster-to-pollster variability. It can no longer be calculated from $p$ or $d$ and is an unknown parameter.
- The central limit theorem still works to estimate the sample average of many polls $X_1,...,X_N$ because the average of the sum of many random variables is a normally distributed random variable with expected value $d$ and standard error $\sigma/\sqrt{N}$.
- We can estimate the unobserved $\sigma$ as the sample standard deviation, which is calculated with the `sd` function.

**Code:**   
Note that to compute the exact 95% confidence interval, we would use `qnorm(.975)` instead of 1.96.  
```{r}
# collect last result before the election for each pollster
one_poll_per_pollster <- polls %>% group_by(pollster) %>%
    filter(enddate == max(enddate)) %>%      # keep latest poll
    ungroup()

# histogram of spread estimates
one_poll_per_pollster %>%
    ggplot(aes(spread)) + geom_histogram(binwidth = 0.01)

# construct 95% confidence interval
results <- one_poll_per_pollster %>%
    summarize(avg = mean(spread), se = sd(spread)/sqrt(length(spread))) %>%
    mutate(start = avg - 1.96*se, end = avg + 1.96*se)
round(results*100, 1)
```

# Section 5: Bayesian Statistics
In Section 5, you will learn about Bayesian statistics through looking at examples from rare disease diagnosis and baseball.

After completing Section 5, you will be able to:

- Apply Bayes' theorem to calculate the probability of A given B.
- Understand how to use hierarchical models to make better predictions by considering multiple levels of variability.
- Compute a posterior probability using an empirical Bayesian approach.
- Calculate a 95% credible interval from a posterior probability.

There are 2 assignments: one that uses the DataCamp platform for you to practice your coding skills, and one on the edX platform for you to apply Bayesian statistics in a real-world context.

We encourage you to use R to interactively test out your answers and further your learning.

## Bayesian Statistics
**Textbook link**  
This video corresponds to the [textbook introduction to the Bayesian statistics section.](https://rafalab.github.io/dsbook/models.html#bayesian-statistics)

**Key points**  

- In the urn model, it does not make sense to talk about the probability of $p$ being greater than a certain value because $p$ is a fixed value.
- With Bayesian statistics, we assume that $p$ is in fact random, which allows us to calculate probabilities related to $p$.
- Hierarchical models describe variability at different levels and incorporate all these levels into a model for estimating $p$.

## Bayes' Theorem
**Textbook link**  
This video corresponds to the [textbook section on Bayes' Theorem.](https://rafalab.github.io/dsbook/models.html#bayes-theorem)

**Key points**  

- Bayes' Theorem states that the probability of event A happening given event B is equal to the probability of both A and B divided by the probability of event B:
\[Pr(A∣B)=\frac{Pr(B∣A)Pr(A)}{Pr(B)} \]
- Bayes' Theorem shows that a test for a very rare disease will have a high percentage of false positives even if the accuracy of the test is high.

**Equations: Cystic fibrosis test probabilities**  
In these probabilities, + represents a positive test, - represents a negative test, $D=0$ indicates no disease, and $D=1$ indicates the disease is present.  

Probability of having the disease given a positive test: $Pr(D=1∣+)$  
99% test accuracy when disease is present: $Pr(+∣D=1)=0.99$  
99% test accuracy when disease is absent: $Pr(−∣D=0)=0.99$  
Rate of cystic fibrosis: $Pr(D=1)=0.00025$  

Bayes' theorem can be applied like this:  

\[Pr(D=1∣+)=\frac{Pr(+∣D=1)⋅Pr(D=1)}{Pr(+)}\]
\[Pr(D=1∣+)=\frac{Pr(+∣D=1)⋅Pr(D=1)}{Pr(+∣D=1)⋅Pr(D=1)+Pr(+∣D=0)⋅Pr(D=0)}\]

Substituting known values, we obtain:  

\[\frac{0.99⋅0.0002}{50.99⋅0.00025+0.01⋅0.99975}=0.02 \]

**Code: Monte Carlo simulation**  
```{r}
prev <- 0.00025    # disease prevalence
N <- 100000    # number of tests
outcome <- sample(c("Disease", "Healthy"), N, replace = TRUE, prob = c(prev, 1-prev))

N_D <- sum(outcome == "Disease")    # number with disease
N_H <- sum(outcome == "Healthy")    # number healthy

# for each person, randomly determine if test is + or -
accuracy <- 0.99
test <- vector("character", N)
test[outcome == "Disease"] <- sample(c("+", "-"), N_D, replace=TRUE, prob = c(accuracy, 1-accuracy))
test[outcome == "Healthy"] <- sample(c("-", "+"), N_H, replace=TRUE, prob = c(accuracy, 1-accuracy))

table(outcome, test)
```

## Bayes in Practice
**Textbook link**  
This video corresponds to the [textbook section on Bayes in practice.](https://rafalab.github.io/dsbook/models.html#bayes-in-practice)

**Key points**  

- The techniques we have used up until now are referred to as *frequentist statistics* as they consider only the frequency of outcomes in a dataset and do not include any outside information. Frequentist statistics allow us to compute confidence intervals and p-values.
- Frequentist statistics can have problems when sample sizes are small and when the data are extreme compared to historical results.
- *Bayesian statistics* allows prior knowledge to modify observed results, which alters our conclusions about event probabilities.

## The Hierarchical Model
**Textbook link**  
This video corresponds to the [textbook section on hierarchical models.](https://rafalab.github.io/dsbook/models.html#hierarchical-models)

**Key points**  

- Hierarchical models use multiple levels of variability to model results. They are hierarchical because values in the lower levels of the model are computed using values from higher levels of the model.
- We model baseball player batting average using a hierarchical model with two levels of variability:
  + $p∼N(\mu,\tau)$ describes player-to-player variability in natural ability to hit, which has a mean $\mu$ and standard deviation $\tau$.
  + $Y∣p∼N(p,\sigma)$ describes a player's observed batting average given their ability $p$, which has a mean $p$ and standard deviation $\sigma=\sqrt{p(1-p)/N}$. This represents variability due to luck.
  + In Bayesian hierarchical models, the first level is called the *prior distribution* and the second level is called the *sampling distribution*.
- The *posterior distribution* allows us to compute the probability distribution of $p$ given that we have observed data $Y$.
- By the continuous version of Bayes' rule, the *expected value of the posterior distribution* $p$ given $Y=y$ is a weighted average between the prior mean $\mu$ and the observed data $Y$:
\[E(p∣y)=B\mu+(1−B)Y\ \ \ \ \ \ where\ \ \ \ B=\frac{σ^2}{σ2+τ2}\]
- The *standard error of the posterior distribution* $SE(p∣Y)^2$ is $\frac{1}{1/σ^2+1/τ^2}$. Note that you will need to take the square root of both sides to solve for the standard error.
- This Bayesian approach is also known as *shrinking*. When $\sigma$ is large, $B$ is close to 1 and our prediction of $p$ shrinks towards the mean ($\mu$). When $\sigma$ is small, $B$ is close to 0 and our prediction of $p$ is more weighted towards the observed data $Y$.

# Section 6: Election Forecasting
In Section 6, you will learn about election forecasting, building on what you've learned in the previous sections about statistical modeling and Bayesian statistics.

After completing Section 6, you will be able to:

- Understand how pollsters use hierarchical models to forecast the results of elections.
- Incorporate multiple sources of variability into a mathematical model to make predictions.
- Construct confidence intervals that better model deviations such as those seen in election data using the t-distribution.

There are 2 assignments that use the DataCamp platform for you to practice your coding skills.

We encourage you to use R to interactively test out your answers and further your learning.

## Election Forecasting
**Textbook link**  
This video corresponds to the [textbook section on the Bayesian approach to election forecasting.](https://rafalab.github.io/dsbook/models.html#bayesian-approach)

**Key points**  

- In our model:
  + The spread $d∼N(\mu,\tau)$ describes our best guess in the absence of polling data. We set $\mu=0$ and $\tau=0.035$ using historical data.
  + The average of observed data $\bar{X}∣d∼N(d,\sigma)$ describes randomness due to sampling and the pollster effect.
- Because the posterior distribution is normal, we can report a 95% *credible interval* that has a 95% chance of overlapping the parameter using $E(p∣Y)$ and $SE(p∣Y)$.
- Given an estimate of $E(p∣Y)$ and $SE(p∣Y)$, we can use  `pnorm` to compute the probability that $d>0$.
- It is common to see a general bias that affects all pollsters in the same way. This bias cannot be predicted or measured before the election. We will include a term in later models to account for this variability.

**Code: Definition of results object**  
This code from previous videos defines the `results` object used for empirical Bayes election forecasting.

```{r}
library(tidyverse)
library(dslabs)
polls <- polls_us_election_2016 %>%
    filter(state == "U.S." & enddate >= "2016-10-31" &
                 (grade %in% c("A+", "A", "A-", "B+") | is.na(grade))) %>%
    mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)

one_poll_per_pollster <- polls %>% group_by(pollster) %>%
    filter(enddate == max(enddate)) %>%
    ungroup()

results <- one_poll_per_pollster %>%
    summarize(avg = mean(spread), se = sd(spread)/sqrt(length(spread))) %>%
    mutate(start = avg - 1.96*se, end = avg + 1.96*se)
```

**Code: Computing the posterior mean, standard error, credible interval and probability**  
Note that to compute an exact 95% credible interval, we would use `qnorm(.975)` instead of 1.96.
```{r}
mu <- 0
tau <- 0.035
sigma <- results$se
Y <- results$avg
B <- sigma^2 / (sigma^2 + tau^2)

posterior_mean <- B*mu + (1-B)*Y
posterior_se <- sqrt(1 / (1/sigma^2 + 1/tau^2))

posterior_mean
posterior_se

# 95% credible interval
posterior_mean + c(-1.96, 1.96)*posterior_se

# probability of d > 0
1 - pnorm(0, posterior_mean, posterior_se)
```

## Mathematical Representations of Models
**Textbook link**  
This video corresponds to the [textbook section on mathematical representations of models.](https://rafalab.github.io/dsbook/models.html#mathematical-representations-of-models)

**Key points**  

- If we collect several polls with measured spreads $X_1,...,X_j$ with a sample size of $N$, these random variables have expected value $d$ and standard error $2\sqrt{p(1−p)/N}$.
- We represent each measurement as $X_{i,j}=d+b+h_i+\epsilon_{i,j}$ where:
  - The index $i$ represents the different pollsters
  - The index $j$ represents the different polls
  - $X_{i,j}$ is the $j$th poll by the $i$th pollster 
  - $d$ is the actual spread of the election
  - $b$ is the general bias affecting all pollsters
  - $h_i$ represents the house effect for the $i$th pollster
  - $\epsilon_{i,j}$ represents the random error associated with the $i,j$th poll.
- The sample average is now $\bar{X}=d+b+\frac{1}{N}\sum_{i=1}^{N}X_i$ with standard deviation $SE(\bar{X})=\sqrt{\sigma^2/N+\sigma^2_b}$.
- The standard error of the general bias $\sigma_b$ does not get reduced by averaging multiple polls, which increases the variability of our final estimate.

**Code: Simulated data with** $\mathbf{X_j=d+\epsilon_j}$
```{r}
J <- 6
N <- 2000
d <- .021
p <- (d+1)/2
X <- d + rnorm(J, 0, 2*sqrt(p*(1-p)/N))
```

**Code: Simulated data with** $\mathbf{X_{i,j}=d+\epsilon_{i,j}}$  
```{r}
I <- 5
J <- 6
N <- 2000
d <- .021
p <- (d+1)/2
X <- sapply(1:I, function(i){
    d + rnorm(J, 0, 2*sqrt(p*(1-p)/N))
})
```

**Code: Simulated data with** $\mathbf{X_{i,j}=d+h_i+\epsilon_{i,j}}$
```{r}
I <- 5
J <- 6
N <- 2000
d <- .021
p <- (d+1)/2
h <- rnorm(I, 0, 0.025)    # assume standard error of pollster-to-pollster variability is 0.025
X <- sapply(1:I, function(i){
    d + rnorm(J, 0, 2*sqrt(p*(1-p)/N))
})
```

**Code: Calculating probability of** $\mathbf{d>0}$ **with general bias**  
Note that `sigma` now includes an estimate of the variability due to general bias  $\sigma_b=.025$.

```{r}
mu <- 0
tau <- 0.035
sigma <- sqrt(results$se^2 + .025^2)
Y <- results$avg
B <- sigma^2 / (sigma^2 + tau^2)
posterior_mean <- B*mu + (1-B)*Y
posterior_se <- sqrt(1 / (1/sigma^2 + 1/tau^2))
1 - pnorm(0, posterior_mean, posterior_se)
```

## Predicting the Electoral College
**Textbook link**  
This video corresponds to the [textbook section on predicting the electoral college.](https://rafalab.github.io/dsbook/models.html#predicting-the-electoral-college)

**Key points**  

- In the US election, each state has a certain number of votes that are won all-or-nothing based on the popular vote result in that state (with minor exceptions not discussed here).
- We use the `left_join` function to combine the number of electoral votes with our poll results.
- For each state, we apply a Bayesian approach to generate an Election Day $d$. We keep our prior simple by assuming an expected value of 0 and a standard deviation based on recent history of 0.02.
- We can run a Monte Carlo simulation that for each iteration simulates poll results in each state using that state's average and standard deviation, awards electoral votes for each state to Clinton if the spread is greater than 0, then compares the number of electoral votes won to the number of votes required to win the election (over 269).
- If we run a Monte Carlo simulation for the electoral college without accounting for general bias, we overestimate Clinton's chances of winning at over 99%.
- If we include a general bias term, the estimated probability of Clinton winning decreases significantly.

**Code: Top 5 states ranked by electoral votes**  
The `results_us_election_2016` object is defined in the `dslabs` package:

```{r}
library(tidyverse)
library(dslabs)
data("polls_us_election_2016")
head(results_us_election_2016)
results_us_election_2016 %>% arrange(desc(electoral_votes)) %>% top_n(5, electoral_votes)
```

**Code: Computing the average and standard deviation for each state**  
```{r}
results <- polls_us_election_2016 %>%
    filter(state != "U.S." &
            !grepl("CD", "state") &
            enddate >= "2016-10-31" &
            (grade %in% c("A+", "A", "A-", "B+") | is.na(grade))) %>%
    mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) %>%
    group_by(state) %>%
    summarize(avg = mean(spread), sd = sd(spread), n = n()) %>%
    mutate(state = as.character(state))
# 10 closest races = battleground states
results %>% arrange(abs(avg))
# joining electoral college votes and results
results <- left_join(results, results_us_election_2016, by="state")
# states with no polls: note Rhode Island and District of Columbia = Democrat
results_us_election_2016 %>% filter(!state %in% results$state)
# assigns sd to states with just one poll as median of other sd values
results <- results %>%
    mutate(sd = ifelse(is.na(sd), median(results$sd, na.rm = TRUE), sd))
```

**Code: Calculating the posterior mean and posterior standard error**  
Note there is a small error in the video code: `B` should be defined as `sigma^2/(sigma^2 + tau^2)`.

```{r}
mu <- 0
tau <- 0.02
# The original code commented below returns an error
#results %>% mutate(sigma = sd/sqrt(n), 
results %>% mutate(sigma = se,
                   B = sigma^2/ (sigma^2 + tau^2),
                   posterior_mean = B*mu + (1-B)*avg,
                   posterior_se = sqrt( 1 / (1/sigma^2 + 1/tau^2))) %>%
    arrange(abs(posterior_mean))
```

**Code: Monte Carlo simulation of Election Night results (no general bias)**  
```{r}
mu <- 0
tau <- 0.02
clinton_EV <- replicate(1000, {
    # The original code commented below returns an error
    #results %>% mutate(sigma = sd/sqrt(n),
    results %>% mutate(sigma = se,
                       B = sigma^2/ (sigma^2 + tau^2),
                       posterior_mean = B*mu + (1-B)*avg,
                       posterior_se = sqrt( 1 / (1/sigma^2 + 1/tau^2)),
                       simulated_result = rnorm(length(posterior_mean), posterior_mean, posterior_se),
                       clinton = ifelse(simulated_result > 0, electoral_votes, 0)) %>%    # award votes if Clinton wins state
        summarize(clinton = sum(clinton)) %>%    # total votes for Clinton
        .$clinton + 7    # 7 votes for Rhode Island and DC
})
mean(clinton_EV > 269)    # over 269 votes wins election
# histogram of outcomes
data.frame(clinton_EV) %>%
    ggplot(aes(clinton_EV)) +
    geom_histogram(binwidth = 1) +
    geom_vline(xintercept = 269)
```

**Code: Monte Carlo simulation including general bias**  
```{r}
mu <- 0
tau <- 0.02
bias_sd <- 0.03
clinton_EV_2 <- replicate(1000, {
    results %>% mutate(sigma = sqrt(sd^2/(n) + bias_sd^2),    # added bias_sd term
                        B = sigma^2/ (sigma^2 + tau^2),
                        posterior_mean = B*mu + (1-B)*avg,
                        posterior_se = sqrt( 1 / (1/sigma^2 + 1/tau^2)),
                        simulated_result = rnorm(length(posterior_mean), posterior_mean, posterior_se),
                        clinton = ifelse(simulated_result > 0, electoral_votes, 0)) %>%    # award votes if Clinton wins state
        summarize(clinton = sum(clinton)) %>%    # total votes for Clinton
        .$clinton + 7    # 7 votes for Rhode Island and DC
})
mean(clinton_EV_2 > 269)    # over 269 votes wins election
```

## Forecasting
**Textbook link**  
This video corresponds to the [textbook section on forecasting.](https://rafalab.github.io/dsbook/models.html#forecasting)

**Key points**  

- In poll results, $p$ is not fixed over time. Variability within a single pollster comes from time variation.
- In order to forecast, our model must include a bias term $b_t$ to model the time effect.
- Pollsters also try to estimate $f(t)$, the trend of $p$ given time $t$ using a model like:
\[Y_{i,j,t}=d+b+h_j+b_t+f(t)+\epsilon_{i,j,t}\]
- Once we decide on a model, we can use historical data and current data to estimate the necessary parameters to make predictions.

**Code: Variability across one pollster**
```{r}
# select all national polls by one pollster
one_pollster <- polls_us_election_2016 %>%
    filter(pollster == "Ipsos" & state == "U.S.") %>%
    mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)
# the observed standard error is higher than theory predicts
se <- one_pollster %>%
    summarize(empirical = sd(spread),
            theoretical = 2*sqrt(mean(spread)*(1-mean(spread))/min(samplesize)))
se
# the distribution of the data is not normal
one_pollster %>% ggplot(aes(spread)) +
    geom_histogram(binwidth = 0.01, color = "black")
```

**Code: Trend across time for several pollsters**
```{r}
polls_us_election_2016 %>%
    filter(state == "U.S." & enddate >= "2016-07-01") %>%
    group_by(pollster) %>%
    filter(n() >= 10) %>%
    ungroup() %>%
    mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) %>%
    ggplot(aes(enddate, spread)) +
    geom_smooth(method = "loess", span = 0.1) +
    geom_point(aes(color = pollster), show.legend = FALSE, alpha = 0.6)
```

**Code: Plotting raw percentages across time**
```{r}
polls_us_election_2016 %>%
    filter(state == "U.S." & enddate >= "2016-07-01") %>%
    select(enddate, pollster, rawpoll_clinton, rawpoll_trump) %>%
    rename(Clinton = rawpoll_clinton, Trump = rawpoll_trump) %>%
    gather(candidate, percentage, -enddate, -pollster) %>%
    mutate(candidate = factor(candidate, levels = c("Trump", "Clinton"))) %>%
    group_by(pollster) %>%
    filter(n() >= 10) %>%
    ungroup() %>%
    ggplot(aes(enddate, percentage, color = candidate)) +
    geom_point(show.legend = FALSE, alpha = 0.4) +
    geom_smooth(method = "loess", span = 0.15) +
    scale_y_continuous(limits = c(30, 50))
```

## The t-Distribution
**Textbook link**  
This video corresponds to the [textbook section on the t-distribtution.](https://rafalab.github.io/dsbook/models.html#t-dist)

**Correction**
At 1:31-1:41, there is an error in the t-distribution formula. The formula should be:

\[Z=\frac{\bar{X}−d}{s/\sqrt{N}}\]

**Key points**  

- In models where we must estimate two parameters, $p$ and $\sigma$, the Central Limit Theorem can result in overconfident confidence intervals for sample sizes smaller than approximately 30.
- If the population data are known to follow a normal distribution, theory tells us how much larger to make confidence intervals to account for estimation of $\sigma$.
- Given $s$ as an estimate of $\sigma$, then $Z=\frac{\bar{X}−d}{s/\sqrt{N}}$ follows a t-distribution with $N−1$ degrees of freedom.
- Degrees of freedom determine the weight of the tails of the distribution. Small values of degrees of freedom lead to increased probabilities of extreme values.
- We can determine confidence intervals using the t-distribution instead of the normal distribution by calculating the desired quantile with the function `qt`.

**Code: Calculating 95% confidence intervals with the t-distribution**
```{r}
z <- qt(0.975, nrow(one_poll_per_pollster) - 1)
one_poll_per_pollster %>%
    summarize(avg = mean(spread), moe = z*sd(spread)/sqrt(length(spread))) %>%
    mutate(start = avg - moe, end = avg + moe)

# quantile from t-distribution versus normal distribution
qt(0.975, 14)    # 14 = nrow(one_poll_per_pollster) - 1
qnorm(0.975)
```

# Section 7: Association Tests
In Section 7, you will learn how to use association and chi-squared tests to perform inference for binary, categorical, and ordinal data through an example looking at research funding rates.

After completing Section 7, you will be able to:

- Use association and chi-squared tests to perform inference on binary, categorical, and ordinal data.
- Calculate an odds ratio to get an idea of the magnitude of an observed effect.

There is 1 assignment that uses the DataCamp platform for you to practice your coding skills.

We encourage you to use R to interactively test out your answers and further your learning.

## Association Tests
**Textbook link**  
This video corresponds to the [textbook section introducing association tests](https://rafalab.github.io/dsbook/inference.html#association-tests) up to and including the [textbook section on two-by-two tables.](https://rafalab.github.io/dsbook/inference.html#two-by-two-tables)

**Key points**  

- We learn how to determine the probability that an observation is due to random variability given categorical, binary or ordinal data.
- Fisher's exact test determines the p-value as the probability of observing an outcome as extreme or more extreme than the observed outcome given the null distribution.
- Data from a binary experiment are often summarized in two-by-two tables.
- The p-value can be calculated from a two-by-two table using Fisher's exact test with the function `fisher.test`.  

**Code: Research funding rates example**
```{r}
# load and inspect research funding rates object
library(tidyverse)
library(dslabs)
data(research_funding_rates)
research_funding_rates
# compute totals that were successful or not successful
totals <- research_funding_rates %>%
    select(-discipline) %>%
    summarize_all(funs(sum)) %>%
    summarize(yes_men = awards_men,
                         no_men = applications_men - awards_men,
                         yes_women = awards_women,
                         no_women = applications_women - awards_women)
# compare percentage of men/women with awards
totals %>% summarize(percent_men = yes_men/(yes_men + no_men),
                                          percent_women = yes_women/(yes_women + no_women))
```

**Code: Two-by-two table and p-value for the Lady Tasting Tea problem**
```{r}
tab <- matrix(c(3,1,1,3), 2, 2)
rownames(tab) <- c("Poured Before", "Poured After")
colnames(tab) <- c("Guessed Before", "Guessed After")
tab
# p-value calculation with Fisher's Exact Test
fisher.test(tab, alternative = "greater")
```

## Chi-Squared Tests
**Textbook links**  
This video corresponds to the [textbook section on the chi-square test](https://rafalab.github.io/dsbook/inference.html#chi-square-test) and the [textbook section on the odds ratio.](https://rafalab.github.io/dsbook/inference.html#odds-ratio)

**Key points**  

- If the sums of the rows and the sums of the columns in the two-by-two table are fixed, then the hypergeometric distribution and  Fisher's exact test can be used. Otherwise, we must use the chi-squared test.
- The *chi-squared test* compares the observed two-by-two table to the two-by-two table expected by the null hypothesis and asks how likely it is that we see a deviation as large as observed or larger by chance.
- The function `chisq.test` takes a two-by-two table and returns the p-value from the chi-squared test.
- The *odds ratio* states how many times larger the odds of an outcome are for one group relative to another group.
- A small p-value does not imply a large odds ratio. If a finding has a small p-value but also a small odds ratio, it may not be a practically significant or scientifically significant finding. 
- Because the odds ratio is a ratio of ratios, there is no simple way to use the Central Limit Theorem to compute confidence intervals. There are advanced methods for computing confidence intervals for odds ratios that we do not discuss here.  

**Code: Chi-squared test**
```{r}
# compute overall funding rate
funding_rate <- totals %>%
    summarize(percent_total = (yes_men + yes_women) / (yes_men + no_men + yes_women + no_women)) %>%
    .$percent_total
funding_rate
# construct two-by-two table for observed data
two_by_two <- tibble(awarded = c("no", "yes"),
                     men = c(totals$no_men, totals$yes_men),
                     women = c(totals$no_women, totals$yes_women))
two_by_two
# compute null hypothesis two-by-two table
tibble(awarded = c("no", "yes"),
           men = (totals$no_men + totals$yes_men) * c(1-funding_rate, funding_rate),
           women = (totals$no_women + totals$yes_women) * c(1-funding_rate, funding_rate))
# chi-squared test
chisq_test <- two_by_two %>%
    select(-awarded) %>%
    chisq.test()
chisq_test$p.value
```

**Code: Odds ratio**
```{r}
# odds of getting funding for men
odds_men <- (two_by_two$men[2] / sum(two_by_two$men)) /
        (two_by_two$men[1] / sum(two_by_two$men))
# odds of getting funding for women
odds_women <- (two_by_two$women[2] / sum(two_by_two$women)) /
        (two_by_two$women[1] / sum(two_by_two$women))
# odds ratio - how many times larger odds are for men than women
odds_men/odds_women
```

**Code: p-value and odds ratio responses to increasing sample size**
```{r}
# multiplying all observations by 10 decreases p-value without changing odds ratio
two_by_two %>%
 select(-awarded) %>%
 mutate(men = men*10, women = women*10) %>%
 chisq.test()
```



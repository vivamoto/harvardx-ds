---
title: "DS07 - Linear Regression"
author: "Victor Ivamoto"
date: "05/11/2019"
output: 
  html_document: 
    toc: true
    toc_float: true
    number_sections: true
    code_folding: show    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Section 1: Introduction to Regression
## Introduction to Regression Overview
In the **Introduction to Regression** section, you will learn the basics of linear regression.

After completing this section, you will be able to:

- Understand how Galton developed **linear regression**.
- Calculate and interpret the **sample correlation**.
- **Stratify** a dataset when appropriate.
- Understand what a **bivariate normal distribution** is.
- Explain what the term **variance explained** means.
- Interpret the two **regression lines**.

This section has three parts: **Baseball as a Motivating Example**, **Correlation**, and **Stratification and Variance Explained**. There are comprehension checks at the end of each part.

We encourage you to use R to interactively test out your answers and further your own learning. If you get stuck, we encourage you to search the discussion boards for the answer to your issue or ask us for help!

## 1.1: Baseball as a Motivating Example
### Motivating Example: Moneyball
**Textbook link**  
The corresponding section of the textbook is the [case study on Moneyball.](https://rafalab.github.io/dsbook/linear-models.html#case-study-moneyball)

**Key points**

- Bill James was the originator of the **sabermetrics**, the approach of using data to predict what outcomes best predicted if a team would win

### Baseball Basics
**Textbook link**  
The corresponding textbook section on [baseball basics can be found here.](https://rafalab.github.io/dsbook/linear-models.html#baseball-basics)

**Key points**  

- The goal of a baseball game is to score more runs (points) than the other team.
- Each team has 9 batters who have an opportunity to hit a ball with a bat in a predetermined order. 
- Each time a batter has an opportunity to bat, we call it a plate appearance (PA).
- The PA ends with a binary outcome: the batter either makes an out (failure) and returns to the bench or the batter doesn’t (success) and can run around the bases, and potentially score a run (reach all 4 bases).
- We are simplifying a bit, but there are five ways a batter can succeed (not make an out):
  1. Bases on balls (BB): the pitcher fails to throw the ball through a predefined area considered to be hittable (the strike zone), so the batter is permitted to go to first base.
  1. Single: the batter hits the ball and gets to first base.
  1. Double (2B): the batter hits the ball and gets to second base.
  1. Triple (3B): the batter hits the ball and gets to third base.
  1. Home Run (HR): the batter hits the ball and goes all the way home and scores a run.
- Historically, the batting average has been considered the most important offensive statistic. To define this average, we define a hit (H) and an at bat (AB). Singles, doubles, triples and home runs are hits. The fifth way to be successful, a walk (BB), is not a hit. An AB is the number of times you either get a hit or make an out; BBs are excluded. The batting average is simply H/AB and is considered the main measure of a success rate.

### Bases on Balls or Stolen Bases?
**Textbook link**  
The corresponding section of the textbook is [base on balls or stolen bases.](https://rafalab.github.io/dsbook/linear-models.html#base-on-balls-or-stolen-bases)

**Key points**

- The visualization of choice when exploring the relationship between two variables like home runs and runs is a scatterplot.

**Code**  
Scatterplot of the relationship between HRs and wins
```{r}
library(Lahman)
library(tidyverse)
library(dslabs)
ds_theme_set()

Teams %>% filter(yearID %in% 1961:2001) %>%
    mutate(HR_per_game = HR / G, R_per_game = R / G) %>%
    ggplot(aes(HR_per_game, R_per_game)) + 
    geom_point(alpha = 0.5)
```

Scatterplot of the relationship between stolen bases and wins
```{r}
Teams %>% filter(yearID %in% 1961:2001) %>%
    mutate(SB_per_game = SB / G, R_per_game = R / G) %>%
    ggplot(aes(SB_per_game, R_per_game)) + 
    geom_point(alpha = 0.5)
```

Scatterplot of the relationship between bases on balls and runs
```{r}
Teams %>% filter(yearID %in% 1961:2001) %>%
    mutate(BB_per_game = BB / G, R_per_game = R / G) %>%
    ggplot(aes(BB_per_game, R_per_game)) + 
    geom_point(alpha = 0.5)
```

### Assessment: Baseball as a Motivating Example
#### Question 1
What is the application of statistics and data science to baseball called?

- Moneyball
- **Sabermetrics**
- The “Oakland A’s Approach”
- There is no specific name for this; it’s just data science.

**Answer**  
The term “sabermetrics” was coined by Bill James, and is derived from the acronym SABR: the society for American baseball research.

#### Question 2
Which of the following outcomes is not included in the batting average?

- A home run
- **A base on balls**
- An out
- A single

**Answer**  
A base on balls is not considered a hit and is excluded from the at-bat total.

#### Question 3
Why do we consider team statistics as well as individual player statistics?

- **The success of any individual player also depends on the strength of their team.**
- Team statistics can be easier to calculate.
- The ultimate goal of sabermetrics is to rank teams, not players.

**Explanation**  
Team statistics are important because the success of individual players depends also on the strength of their team.

#### Question 4
You want to know whether teams with more at-bats per game have more runs per game.

What R code below correctly makes a scatter plot for this relationship?

- 
```
Teams %>% filter(yearID %in% 1961:2001 ) %>%
    ggplot(aes(AB, R)) + 
    geom_point(alpha = 0.5)
```
- **`Teams %>% filter(yearID %in% 1961:2001 ) %>%`**  
**`    mutate(AB_per_game = AB/G, R_per_game = R/G) %>%`**  
**`    ggplot(aes(AB_per_game, R_per_game)) + `**  
**`    geom_point(alpha = 0.5)`**  

- 
```
Teams %>% filter(yearID %in% 1961:2001 ) %>%
    mutate(AB_per_game = AB/G, R_per_game = R/G) %>%
    ggplot(aes(AB_per_game, R_per_game)) + 
    geom_line()
```
- 
```
Teams %>% filter(yearID %in% 1961:2001 ) %>%
    mutate(AB_per_game = AB/G, R_per_game = R/G) %>%
    ggplot(aes(R_per_game, AB_per_game)) + 
    geom_point()
```

**Answer**  
This makes a scatter plot of runs per game (y-axis) vs. at-bats per game (x-axis).

#### Question 5
What does the variable “SOA” stand for in the `Teams` table?
Hint: make sure to use the help file (`?Teams`).

- sacrifice out
- slides or attempts
- **strikeouts by pitchers**  
- accumulated singles

#### Question 6
Load the **Lahman** library. Filter the `Teams` data frame to include years from 1961 to 2001. Make a scatterplot of runs per game versus at bats (`AB`) per game.

Which of the following is true?

- There is no clear relationship between runs and at bats per game.
- **As the number of at bats per game increases, the number of runs per game tends to increase.**  
- As the number of at bats per game increases, the number of runs per game tends to decrease.

**Explanation**  
When you examine the scatterplot, you can see a clear trend towards increasing runs per game with increasing number of at-bats per game. The following code can be used to make the scatterplot:

```{r}
library(Lahman)
library(tidyverse)
library(dslabs)
Teams %>% filter(yearID %in% 1961:2001) %>%
  mutate(AB_per_game = AB / G, R_per_game = R / G) %>%
  ggplot(aes(AB_per_game, R_per_game)) + 
  geom_point(alpha = 0.5)
```

#### Question 7
Use the filtered `Teams` data frame from Question 6. Make a scatterplot of win rate (number of wins per game) versus number of fielding errors (`E`) per game.

Which of the following is true?

- There is no clear relationship between win rate and errors per game.
- As the number of errors per game increases, the win rate tends to increase.
- **As the number of errors per game increases, the win rate tends to decrease.**  

**Explanation**  
When you examine the scatterplot, you can see a clear trend towards decreased win rate with increasing number of errors per game. The following code can be used to make the scatterplot:

```{r}
Teams %>% filter(yearID %in% 1961:2001) %>%
  mutate(win_rate = W / G, E_per_game = E / G) %>%
  ggplot(aes(win_rate, E_per_game)) + 
  geom_point(alpha = 0.5)
```

#### Question 8
Use the filtered `Teams` data frame from Question 6. Make a scatterplot of triples (`X3B`) per game versus doubles (`X2B`) per game.

Which of the following is true?

- **There is no clear relationship between doubles per game and triples per game.**  
- As the number of doubles per game increases, the number of triples per game tends to increase.
- As the number of doubles per game increases, the number of triples per game tends to decrease.

**Explanation**  
When you examine the scatterplot, you can see no clear relationship between doubles and triples per game. The following code can be used to make the scatterplot:

```{r}
Teams %>% filter(yearID %in% 1961:2001) %>%
  mutate(doubles_per_game = X2B / G, triples_per_game = X3B / G) %>%
  ggplot(aes(doubles_per_game, triples_per_game)) + 
  geom_point(alpha = 0.5)
```

## 1.2: Correlation
### Correlation
**Textbook link**  
The corresponding textbook section is the [Case Study: is height hereditary?](https://rafalab.github.io/dsbook/regression.html#case-study-is-height-hereditary)

**Key points**

- Galton tried to predict sons' heights based on fathers' heights.
- The mean and standard errors are insufficient for describing an important characteristic of the data: the trend that the taller the father, the taller the son.
- The correlation coefficient is an informative summary of how two variables move together that can be used to predict one variable using the other.

**Code**
```{r}
# create the dataset
library(tidyverse)
library(HistData)
data("GaltonFamilies")
set.seed(1983)
galton_heights <- GaltonFamilies %>%
  filter(gender == "male") %>%
  group_by(family) %>%
  sample_n(1) %>%
  ungroup() %>%
  select(father, childHeight) %>%
  rename(son = childHeight)

# means and standard deviations
galton_heights %>%
    summarize(mean(father), sd(father), mean(son), sd(son))

# scatterplot of father and son heights
galton_heights %>%
    ggplot(aes(father, son)) +
    geom_point(alpha = 0.5)
```

### Correlation Coefficient
**Textbook link**  
The [correlation coefficient section of the textbook can be found here.](https://rafalab.github.io/dsbook/regression.html#the-correlation-coefficient)

**Key points**  

- The correlation coefficient is defined for a list of pairs $(x_1,y_1),...,(x_n,y_n)$ as the product of the standardized values: $\left ( \frac{x_i−\mu_x}{\sigma_x}\right )\left (\frac{y_i−\mu_y}{\sigma_y}\right )$.
- The correlation coefficient essentially conveys how two variables move together.
- The correlation coefficient is always between -1 and 1.

**Code**
```{r}
# objects 'x' and 'y' not found
#rho <- mean(scale(x)*scale(y))
galton_heights %>% summarize(r = cor(father, son)) %>% pull(r)
```

### Sample Correlation is a Random Variable
**Textbook link**  
The [sample correlation is a random variable section of the textbook can be found here.](https://rafalab.github.io/dsbook/regression.html#sample-correlation-is-a-random-variable)

**Key points**

- The correlation that we compute and use as a summary is a random variable.
- When interpreting correlations, it is important to remember that correlations derived from samples are estimates containing uncertainty.
- Because the sample correlation is an average of independent draws, the central limit theorem applies. 

**Code**
```{r}
# compute sample correlation
R <- sample_n(galton_heights, 25, replace = TRUE) %>%
    summarize(r = cor(father, son))
R

# Monte Carlo simulation to show distribution of sample correlation
B <- 1000
N <- 25
R <- replicate(B, {
    sample_n(galton_heights, N, replace = TRUE) %>%
    summarize(r = cor(father, son)) %>%
    pull(r)
})
qplot(R, geom = "histogram", binwidth = 0.05, color = I("black"))

# expected value and standard error
mean(R)
sd(R)

# QQ-plot to evaluate whether N is large enough
data.frame(R) %>%
    ggplot(aes(sample = R)) +
    stat_qq() +
    geom_abline(intercept = mean(R), slope = sqrt((1-mean(R)^2)/(N-2)))
```

### Assessment: Correlation
#### Question 1
While studying heredity, Francis Galton developed what important statistical concept?

- Standard deviation
- Normal distribution
- **Correlation**  
- Probability

**Explanation**  
Francis Galton developed the concept of correlation while study heredity.

#### Question 2
The correlation coefficient is a summary of what?

- **The trend between two variables**  
- The dispersion of a variable
- The central tendency of a variable
- The distribution of a variable

#### Question 3
Below is a scatter plot showing the relationship between two variables, x and y.

![](C:/Users/vivam/OneDrive/Documentos/homework-0/Data Science/7_6_1_assessment_IDS.png)

From this figure, the correlation between x and y appears to be about:

- **-0.9**  
- -0.2
- 0.9
- 2

**Answer**  
The variables x and y have a strong negative relationship with each other; as x increases, y decreases.

#### Question 4
Instead of running a Monte Carlo simulation with a sample size of 25 from the 179 father-son pairs described in the videos, we now run our simulation with a sample size of 50.

Would you expect the **mean** of our sample correlation to increase, decrease, or stay approximately the same?

- Increase
- Decrease
- **Stay approximately the same**

**Explanation**  
Because the expected value of the sample correlation is the population correlation, it should stay approximately the same even if the sample size is increased.

#### Question 5
Instead of running a Monte Carlo simulation with a sample size of 25 from the 179 father-son pairs described in the videos, we now run our simulation with a sample size of 50.

Would you expect the **standard deviation** of our sample correlation to increase, decrease, or stay approximately the same?

- Increase
- **Decrease**
- Stay approximately the same

**Explanation**  
As the sample size N increases, the standard deviation of the sample correlation should decrease.

#### Question 6
If X and Y are completely independent, what do you expect the value of the correlation coefficient to be?

- -1
- -0.5
- **0**
- 0.5
- 1
- Not enough information to answer the question

**Answer**  
If X and Y are independent, then their correlation coefficient is 0.

#### Question 7
Load the **Lahman** library. Filter the `Teams` data frame to include years from 1961 to 2001.

What is the correlation coefficient between number of runs per game and number of at bats per game?
```{r}
library(Lahman)
Teams_small <- Teams %>% filter(yearID %in% 1961:2001)
cor(Teams_small$R/Teams_small$G, Teams_small$AB/Teams_small$G)
```

#### Question 8
Use the filtered `Teams` data frame from Question 7.

What is the correlation coefficient between win rate (number of wins per game) and number of errors per game?
```{r}
cor(Teams_small$W/Teams_small$G, Teams_small$E/Teams_small$G)
```

#### Question 9
Use the filtered `Teams` data frame from Question 7.

What is the correlation coefficient between doubles (`X2B`) per game and triples (`X3B`) per game?
```{r}
cor(Teams_small$X2B/Teams_small$G, Teams_small$X3B/Teams_small$G)
```

## 1.3: Stratification and Variance Explained
### Anscombe's Quartet/Stratification
**Textbook link**  
There are three links to relevant sections of the textbook for this video:

- [correlation is not always a useful summary](https://rafalab.github.io/dsbook/regression.html#correlation-is-not-always-a-useful-summary)
- [conditional expectation](https://rafalab.github.io/dsbook/regression.html#conditional-expectation)
- [the regression line](https://rafalab.github.io/dsbook/regression.html#the-regression-line)

**Key points**

- Correlation is not always a good summary of the relationship between two variables.
- The general idea of conditional expectation is that we stratify a population into groups and compute summaries in each group.
- A practical way to improve the estimates of the conditional expectations is to define strata of with similar values of x.
- If there is perfect correlation, the regression line predicts an increase that is the same number of SDs for both variables. If there is 0 correlation, then we don’t use x at all for the prediction and simply predict the average $\mu_y$ . For values between 0 and 1, the prediction is somewhere in between. If the correlation is negative, we predict a reduction instead of an increase.

**Code**
```{r}
# number of fathers with height 72 or 72.5 inches
sum(galton_heights$father == 72)
sum(galton_heights$father == 72.5)

# predicted height of a son with a 72 inch tall father
conditional_avg <- galton_heights %>%
    filter(round(father) == 72) %>%
    summarize(avg = mean(son)) %>%
    pull(avg)
conditional_avg

# stratify fathers' heights to make a boxplot of son heights
galton_heights %>% mutate(father_strata = factor(round(father))) %>%
    ggplot(aes(father_strata, son)) +
    geom_boxplot() +
    geom_point()

# center of each boxplot
galton_heights %>%
    mutate(father = round(father)) %>%
    group_by(father) %>%
    summarize(son_conditional_avg = mean(son)) %>%
    ggplot(aes(father, son_conditional_avg)) +
    geom_point()

# calculate values to plot regression line on original data
mu_x <- mean(galton_heights$father)
mu_y <- mean(galton_heights$son)
s_x <- sd(galton_heights$father)
s_y <- sd(galton_heights$son)
r <- cor(galton_heights$father, galton_heights$son)
m <- r * s_y/s_x
b <- mu_y - m*mu_x

# add regression line to plot
galton_heights %>%
    ggplot(aes(father, son)) +
    geom_point(alpha = 0.5) +
    geom_abline(intercept = b, slope = m)
```

### Bivariate Normal Distribution
**Textbook link**  
The link to the corresponding textbook section is [bivariate normal distribution (advanced).](https://rafalab.github.io/dsbook/regression.html#bivariate-normal-distribution-advanced)

**Key points**

- When a pair of random variables are approximated by the bivariate normal distribution, scatterplots look like ovals. They can be thin (high correlation) or circle-shaped (no correlation).
- When two variables follow a bivariate normal distribution, computing the regression line is equivalent to computing conditional expectations.
- We can obtain a much more stable estimate of the conditional expectation by finding the regression line and using it to make predictions.

**Code**
```{r}
galton_heights %>%
  mutate(z_father = round((father - mean(father)) / sd(father))) %>%
  filter(z_father %in% -2:2) %>%
  ggplot() +  
  stat_qq(aes(sample = son)) +
  facet_wrap( ~ z_father)
```

### Variance Explained
**Correction**  
The equation shown at 0:10 is for the *standard deviation* of the conditional distribution, not the *variance*. The variance is the standard deviation squared. See the notes below the video for more clarification.

**Textbook link**  
The section on [variance explained from the textbook can be found here.](https://rafalab.github.io/dsbook/regression.html#variance-explained)

**Key points**

- Conditioning on a random variable X can help to reduce variance of response variable Y.
- The standard deviation of the conditional distribution is $SD(Y∣X=x)=\sigma_y\sqrt{1−\rho^2}$ , which is smaller than the standard deviation without conditioning $\sigma_y$.
- Because variance is the standard deviation squared, the variance of the conditional distribution is $\sigma^2_y(1−\rho^2)$.
- In the statement "X explains such and such percent of the variability," the percent value refers to the variance. The variance decreases by $\rho^2$ percent.
- The “variance explained” statement only makes sense when the data is approximated by a bivariate normal distribution.

### There are Two Regression Lines
**Textbook link**  
The link to the corresponding section of the textbook is [warning: there are two regression lines.](https://rafalab.github.io/dsbook/regression.html#warning-there-are-two-regression-lines)

**Key point**

- There are two different regression lines depending on whether we are taking the expectation of Y given X or taking the expectation of X given Y.

**Code**
```{r}
# compute a regression line to predict the son's height from the father's height
mu_x <- mean(galton_heights$father)
mu_y <- mean(galton_heights$son)
s_x <- sd(galton_heights$father)
s_y <- sd(galton_heights$son)
r <- cor(galton_heights$father, galton_heights$son)
m_1 <-  r * s_y / s_x
b_1 <- mu_y - m_1*mu_x

# compute a regression line to predict the father's height from the son's height
m_2 <-  r * s_x / s_y
b_2 <- mu_x - m_2*mu_y
```

### Assessment: Stratification and Variance Explained, Part 1
#### Question 1
Look at the figure below.

![](C:/Users/vivam/OneDrive/Documentos/homework-0/Data Science/7_8_1_assessment_IDS.png)

The slope of the regression line in this figure is equal to what, in words?

- **`Slope = (correlation coefficient of son and father heights) * (standard deviation of sons’ heights / standard deviation of fathers’ heights)`**
- 
```
Slope = (correlation coefficient of son and father heights) * (standard deviation of fathers’ heights / standard deviation of sons’ heights)
```
- 
```
Slope = (correlation coefficient of son and father heights) / (standard deviation of sons’ heights * standard deviation of fathers’ heights)
```
- 
```
Slope = (mean height of fathers) - (correlation coefficient of son and father heights * mean height of sons).
```

#### Question 2
Why does the regression line simplify to a line with intercept zero and slope $\rho$ when we standardize our x and y variables?
Try the simplification on your own first!

- When we standardize variables, both x and y will have a mean of one and a standard deviation of zero. When you substitute this into the formula for the regression line, the terms cancel out until we have the following equation: $y_i=\rho x_i$.
- **When we standardize variables, both x and y will have a mean of zero and a standard deviation of one. When you substitute this into the formula for the regression line, the terms cancel out until we have the following equation:** $\mathbf{y_i=\rho x_i}$.
- When we standardize variables, both x and y will have a mean of zero and a standard deviation of one. When you substitute this into the formula for the regression line, the terms cancel out until we have the following equation: $y_i=\rho+x_i$.

**Explanation**  
When we standardize variables, they have a mean of 0 and a standard deviation of 1, giving the equation $y_i=\rho x_i$. The equation $y_i=\rho+x_i$ is for a line with an intercept equal to the correlation coefficient, but the regression line simplifies to have an intercept of 0 when we standardize variables.

#### Question 3
What is a limitation of calculating conditional means?
Select ALL that apply.

- **Each stratum we condition on (e.g., a specific father’s height) may not have many data points.**  
- **Because there are limited data points for each stratum, our average values have large standard errors.**  
- **Conditional means are less stable than a regression line.**  
- Conditional means are a useful theoretical tool but cannot be calculated.
correta

#### Question 4
A regression line is the best prediction of Y given we know the value of X when:

- **X and Y follow a bivariate normal distribution.**
- Both X and Y are normally distributed.
- Both X and Y have been standardized.
- There are at least 25 X-Y pairs.

#### Question 5
Which one of the following scatterplots depicts an x and y distribution that is NOT well-approximated by the bivariate normal distribution?

- **Correct**
![](C:/Users/vivam/OneDrive/Documentos/homework-0/Data Science/7_9_2_assessment_A_IDS.png)
- 
![](C:/Users/vivam/OneDrive/Documentos/homework-0/Data Science/7_9_2_assessment_B_IDS.png)
- 
![](C:/Users/vivam/OneDrive/Documentos/homework-0/Data Science/7_9_2_assessment_C_IDS.png)

- 
![](C:/Users/vivam/OneDrive/Documentos/homework-0/Data Science/7_9_2_assessment_D_IDS.png)

**Explanation**  
The v-shaped distribution of points from the first plot means that the x and y variables do not follow a bivariate normal distribution.
When a pair of random variables is approximated by a bivariate normal, the scatter plot looks like an oval (as in the 2nd, 3rd, and 4th plots) - it is okay if the oval is very round (as in the 3rd plot) or long and thin (as in the 4th plot).

#### Question 6
We previously calculated that the correlation coefficient $\rho$ between fathers’ and sons’ heights is 0.5.

Given this, what percent of the variation in sons’ heights is explained by fathers’ heights?

- 0%
- **25%**
- 50%
- 75%

**Answer**  
When two variables follow a bivariate normal distribution, the variation explained can be calculated as $\rho^2×100$ .

#### Question 7
Suppose the correlation between father and son’s height is 0.5, the standard deviation of fathers’ heights is 2 inches, and the standard deviation of sons’ heights is 3 inches.

Given a one inch increase in a father’s height, what is the predicted change in the son’s height?

- 0.333
- 0.5
- 0.667
- **0.75**
- 1
- 1.5

**Answer**  
The slope of the regression line is calculated by multiplying the correlation coefficient by the ratio of the standard deviation of son heights and standard deviation of father heights: $\sigma_{son}/\sigma_{father}$.

### Assessment: Stratification and Variance Explained, Part 2
In the second part of this assessment, you'll analyze a set of mother and daughter heights, also from **GaltonFamilies**.

Define **`female_heights`**, a set of mother and daughter heights sampled from **GaltonFamilies**, as follows:

```{r}
set.seed(1989, sample.kind="Rounding") #if you are using R 3.6 or later
library(HistData)
data("GaltonFamilies")

female_heights <- GaltonFamilies%>%    
    filter(gender == "female") %>%    
    group_by(family) %>%    
    sample_n(1) %>%    
    ungroup() %>%    
    select(mother, childHeight) %>%    
    rename(daughter = childHeight)
```

#### Question 8
5.0/5.0 points (graded)
Calculate the mean and standard deviation of mothers' heights, the mean and standard deviation of daughters' heights, and the correlaton coefficient between mother and daughter heights.

Mean of mothers' heights
```{r}
mean(female_heights$mother)
```

Standard deviation of mothers' heights
```{r}
sd(female_heights$mother)
```

Mean of daughters' heights
```{r}
mean(female_heights$daughter)
```

Standard deviation of daughters' heights
```{r}
sd(female_heights$daughter)
```

Correlation coefficient
```{r}
cor(female_heights$mother, female_heights$daughter)
```

#### Question 9
Calculate the slope and intercept of the regression line predicting daughters' heights given mothers' heights. Given an increase in mother's height by 1 inch, how many inches is the daughter's height expected to change?

Slope of regression line predicting daughters' height from mothers' heights
```{r}
r <- cor(female_heights$mother, female_heights$daughter)
s_y <- sd(female_heights$daughter)
s_x <- sd(female_heights$mother)
r * s_y/s_x
```

Intercept of regression line predicting daughters' height from mothers' heights
```{r}
mu_y <- mean(female_heights$daughter)
mu_x <- mean(female_heights$mother)
mu_y - (r * s_y/s_x)*mu_x
```

Change in daughter's height in inches given a 1 inch increase in the mother's height
```{r}
r * s_y/s_x
```

#### Question 10
What percent of the variability in daughter heights is explained by the mother's height?
Report your answer as a number between 0 and 100.

```{r}
r^2*100.
```

#### Question 11
A mother has a height of 60 inches.

What is the conditional expected value of her daughter's height given the mother's height?
```{r}
m = r * s_y/s_x
b = mu_y - (r * s_y/s_x)*mu_x
x = 60
m*x+b
```

# Section 2: Linear Models
## Linear Models Overview
In the **Linear Models** section, you will learn how to do linear regression.

After completing this section, you will be able to:

- Use **multivariate regression** to adjust for confounders.
- Write **linear models** to describe the relationship between two or more variables.
- Calculate the **least squares estimates** for a regression model using the lm function.
- Understand the differences between **tibbles** and **data frames**.
- Use the **do** function to bridge R functions and the tidyverse.
- Use the **tidy**, **glance**, and **augment** functions from the **broom** package.
- Apply linear regression to **measurement error models**.

This section has four parts: **Introduction to Linear Models**, **Least Squares Estimates**, **Tibbles, do, and broom**, and **Regression and Baseball**. There are comprehension checks at the end of each part, along with an assessment on linear models at the end of the whole section for Verified learners only.

We encourage you to use R to interactively test out your answers and further your own learning. If you get stuck, we encourage you to search the discussion boards for the answer to your issue or ask us for help!

## 2.1: Introduction to Linear Models
### Confounding: Are BBs More Predictive?
**Textbook link**  
Here is the link to the [textbook section on confounding.](https://rafalab.github.io/dsbook/linear-models.html#confounding)

**Key points**

- Association is not causation!
- Although it may appear that BB cause runs, it is actually the HR that cause most of these runs. We say that BB are **confounded** with HR.
- Regression can help us account for confounding.

**Code**
```{r}
# find regression line for predicting runs from BBs
library(tidyverse)
library(Lahman)
bb_slope <- Teams %>% 
  filter(yearID %in% 1961:2001 ) %>% 
  mutate(BB_per_game = BB/G, R_per_game = R/G) %>% 
  lm(R_per_game ~ BB_per_game, data = .) %>% 
  .$coef %>%
  .[2]
bb_slope

# compute regression line for predicting runs from singles
singles_slope <- Teams %>% 
  filter(yearID %in% 1961:2001 ) %>%
  mutate(Singles_per_game = (H-HR-X2B-X3B)/G, R_per_game = R/G) %>%
  lm(R_per_game ~ Singles_per_game, data = .) %>%
  .$coef  %>%
  .[2]
singles_slope

# calculate correlation between HR, BB and singles
Teams %>% 
  filter(yearID %in% 1961:2001 ) %>% 
  mutate(Singles = (H-HR-X2B-X3B)/G, BB = BB/G, HR = HR/G) %>%  
  summarize(cor(BB, HR), cor(Singles, HR), cor(BB,Singles))
```

### Stratification and Multivariate Regression
**Textbook link**  
Here is the link for the [textbook section on multivariate regression.](https://rafalab.github.io/dsbook/linear-models.html#multivariate-regression)

**Key points**

- A first approach to check confounding is to keep HRs fixed at a certain value and then examine the relationship between BB and runs.
- The slopes of BB after stratifying on HR are reduced, but they are not 0, which indicates that BB are helpful for producing runs, just not as much as previously thought.

**Code**
```{r}
# stratfy HR per game to nearest 10, filter out strata with few points
dat <- Teams %>% filter(yearID %in% 1961:2001) %>%
  mutate(HR_strata = round(HR/G, 1), 
         BB_per_game = BB / G,
         R_per_game = R / G) %>%
  filter(HR_strata >= 0.4 & HR_strata <=1.2)

# scatterplot for each HR stratum
dat %>% 
  ggplot(aes(BB_per_game, R_per_game)) +  
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm") +
  facet_wrap( ~ HR_strata)

# calculate slope of regression line after stratifying by HR
dat %>%  
  group_by(HR_strata) %>%
  summarize(slope = cor(BB_per_game, R_per_game)*sd(R_per_game)/sd(BB_per_game))

# stratify by BB
dat <- Teams %>% filter(yearID %in% 1961:2001) %>%
  mutate(BB_strata = round(BB/G, 1), 
         HR_per_game = HR / G,
         R_per_game = R / G) %>%
  filter(BB_strata >= 2.8 & BB_strata <=3.9) 

# scatterplot for each BB stratum
dat %>% ggplot(aes(HR_per_game, R_per_game)) +  
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm") +
  facet_wrap( ~ BB_strata)

# slope of regression line after stratifying by BB
dat %>%  
  group_by(BB_strata) %>%
  summarize(slope = cor(HR_per_game, R_per_game)*sd(R_per_game)/sd(HR_per_game)) 
```

### Linear Models
**Textbook link**  
Here is the link to the [textbook section on linear models.](https://rafalab.github.io/dsbook/linear-models.html#lse)

**Key points**

- “Linear” here does not refer to lines, but rather to the fact that the conditional expectation is a linear combination of known quantities.
- In Galton's model, we assume $Y$ (son's height) is a linear combination of a constant and $X$ (father's height) plus random noise. We further assume that $\epsilon_i$ are independent from each other, have expected value 0 and the standard deviation $\sigma$ which does not depend on $i$.
- Note that if we further assume that $\epsilon$ is normally distributed, then the model is exactly the same one we derived earlier by assuming bivariate normal data.
- We can subtract the mean from $X$ to make $\beta_0$ more interpretable.  

### Assessment: Introduction to Linear Models
#### Question 1
Why is the number of home runs considered a confounder of the relationship between bases on balls and runs per game?

- Home runs is not a confounder of this relationship.
- Home runs are the primary cause of runs per game.
- The correlation between home runs and runs per game is stronger than the correlation between bases on balls and runs per game.
- **Players who get more bases on balls also tend to have more home runs; in addition, home runs increase the points per game.**

**Explanation**  
Number of home runs is a confounder of the relationship between bases on balls and runs per game because players who get more bases on balls also tend to have more home runs and home runs also increase the points/runs scored per game.


#### Question 2
As described in the videos, when we stratified our regression lines for runs per game vs. bases on balls by the number of home runs, what happened?

- The slope of runs per game vs. bases on balls within each stratum was reduced because we removed confounding by home runs.
- The slope of runs per game vs. bases on balls within each stratum was reduced because there were fewer data points.
- The slope of runs per game vs. bases on balls within each stratum increased after we removed confounding by home runs.
- The slope of runs per game vs. bases on balls within each stratum stayed about the same as the original slope.

**Explanation**  
Home runs are a confounder in the runs per game vs. bases on balls regression analysis. When we removed confounding by home runs, the slope of runs per game vs. bases on balls within each stratum decreased.

#### Question 3
We run a linear model for sons’ heights vs. fathers’ heights using the Galton height data, and get the following results:

```
 > lm(son ~ father, data = galton_heights)

Call:
lm(formula = son ~ father, data = galton_heights)

Coefficients:
(Intercept)    father  
    35.71       0.50  
```
Interpret the numeric coefficient for "father."

- For every inch we increase the son’s height, the predicted father’s height increases by 0.5 inches.
- **For every inch we increase the father’s height, the predicted son’s height grows by 0.5 inches.**
- For every inch we increase the father’s height, the predicted son’s height is 0.5 times greater.

**Explanation**  
The coefficient for “father” gives the predicted increase in son’s height for each increase of 1 unit in the father’s height. In this case, it means that for every inch we increase the father’s height, the son’s predicted height increases by 0.5 inches.

#### Question 4
We want the intercept term for our model to be more interpretable, so we run the same model as before but now we subtract the mean of fathers’ heights from each individual father’s height to create a new variable centered at zero.

```
galton_heights <- galton_heights %>%
    mutate(father_centered=father - mean(father))
```
We run a linear model using this centered fathers’ height variable.

```
> lm(son ~ father_centered, data = galton_heights)

Call:
lm(formula = son ~ father_centered, data = galton_heights)

Coefficients:
(Intercept)    father_centered  
    70.45          0.50  
```
Interpret the numeric coefficient for the intercept.

- **The height of a son of a father of average height is 70.45 inches.**
- The height of a son when a father’s height is zero is 70.45 inches.
- The height of an average father is 70.45 inches.

**Explanation**  
Because the fathers’ heights (the independent variable) have been centered on their mean, the intercept represents the height of the son of a father of average height. In this case, that means that the height of a son of a father of average height is 70.45 inches.

If we had not centered fathers’ heights to its mean, then the intercept would represent the height of a son when a father’s height is zero.

#### Question 5
Suppose we fit a multivariate regression model for expected runs based on BB and HR:

\[E\left [R|BB=x_1,HR=x_2\right ]=\beta_0+\beta_1x_1+\beta_2x_2\] 

Suppose we fix $BB=x_1$. Then we observe a linear relationship between runs and HR with intercept of:

- $\beta_0$ 
- $\beta_0+\beta_2x_2$
- $\mathbf{\beta_0+\beta_1x_1}$ **<==**
- $\beta_0+\beta_2x_1$ 

**Explanation**  
If $x_1$ is fixed, then $\beta_1x_1$ is fixed and acts as the intercept for this regression model. This is the basis of stratificaton.

#### Question 6
Which of the following are assumptions for the errors $\epsilon_i$ in a linear regression model?

- **The** $\mathbf{\epsilon_i}$ **are independent of each other**
- **The** $\mathbf{\epsilon_i}$ **have expected value 0**
- **The variance of** $\mathbf{\epsilon_i}$ **is a constant**

## 2.2: Least Squares Estimates
### Least Squares Estimates (LSE)
**Textbook link**  
The textbook section on [least squares estimates can be found here.](https://rafalab.github.io/dsbook/linear-models.html#lse)

**Key points**

- For regression, we aim to find the coefficient values that minimize the distance of the fitted model to the data.
- Residual sum of squares (RSS) measures the distance between the true value and the predicted value given by the regression line. The values that minimize the RSS are called the least squares estimates (LSE).
- We can use partial derivatives to get the values for $\beta_0$ and $\beta_1$ in Galton's data.

**Code**
```{r}
# compute RSS for any pair of beta0 and beta1 in Galton's data
library(HistData)
data("GaltonFamilies")
set.seed(1983)
galton_heights <- GaltonFamilies %>%
  filter(gender == "male") %>%
  group_by(family) %>%
  sample_n(1) %>%
  ungroup() %>%
  select(father, childHeight) %>%
  rename(son = childHeight)
rss <- function(beta0, beta1, data){
    resid <- galton_heights$son - (beta0+beta1*galton_heights$father)
    return(sum(resid^2))
}

# plot RSS as a function of beta1 when beta0=25
beta1 = seq(0, 1, len=nrow(galton_heights))
results <- data.frame(beta1 = beta1,
                      rss = sapply(beta1, rss, beta0 = 25))
results %>% ggplot(aes(beta1, rss)) + geom_line() + 
  geom_line(aes(beta1, rss))

```

### The lm Function
**Textbook link**  
The textbook section on the **lm** [function can be found here.](https://rafalab.github.io/dsbook/linear-models.html#the-lm-function)

**Key points**

- When calling the **`lm`** function, the variable that we want to predict is put to the left of the ~ symbol, and the variables that we use to predict is put to the right of the ~ symbol. The intercept is added automatically.
- LSEs are random variables.

**Code**
```{r}
# fit regression line to predict son's height from father's height
fit <- lm(son ~ father, data = galton_heights)
fit

# summary statistics
summary(fit)
```

### LSE are Random Variables
**Textbook link**  
The textbook section on [LSE can be found here.](https://rafalab.github.io/dsbook/linear-models.html#lse-are-random-variables)

**Key points**

- Because they are derived from the samples, LSE are random variables.
- $\beta_0$ and $\beta_1$ appear to be normally distributed because the central limit theorem plays a role.
- The t-statistic depends on the assumption that $\epsilon$ follows a normal distribution.

**Code**
```{r}
# Monte Carlo simulation
B <- 1000
N <- 50
lse <- replicate(B, {
  sample_n(galton_heights, N, replace = TRUE) %>% 
    lm(son ~ father, data = .) %>% 
    .$coef 
})
lse <- data.frame(beta_0 = lse[1,], beta_1 = lse[2,]) 

# Plot the distribution of beta_0 and beta_1
library(gridExtra)
p1 <- lse %>% ggplot(aes(beta_0)) + geom_histogram(binwidth = 5, color = "black") 
p2 <- lse %>% ggplot(aes(beta_1)) + geom_histogram(binwidth = 0.1, color = "black") 
grid.arrange(p1, p2, ncol = 2)

# summary statistics
sample_n(galton_heights, N, replace = TRUE) %>% 
  lm(son ~ father, data = .) %>% 
  summary %>%
  .$coef

lse %>% summarize(se_0 = sd(beta_0), se_1 = sd(beta_1))
```

### Advanced Note on LSE
Although interpretation is not straight-forward, it is also useful to know that the LSE can be strongly correlated, which can be seen using this code:

```{r}
lse %>% summarize(cor(beta_0, beta_1))
```

However, the correlation depends on how the predictors are defined or transformed.

Here we standardize the father heights, which changes $x_i$ to $x_i−\bar{x}$.

```{r}
B <- 1000
N <- 50
lse <- replicate(B, {
      sample_n(galton_heights, N, replace = TRUE) %>%
      mutate(father = father - mean(father)) %>%
      lm(son ~ father, data = .) %>% .$coef 
})
```

Observe what happens to the correlation in this case:

```{r}
cor(lse[1,], lse[2,]) 
```

### Predicted Variables are Random Variables
**Textbook link**  
The textbook section on [predicted values can be found here.](https://rafalab.github.io/dsbook/linear-models.html#predicted-values-are-random-variables)

**Key points**

- The predicted value is often denoted as $\hat{Y}$, which is a random variable. Mathematical theory tells us what the standard error of the predicted value is.
- The `predict` function in R can give us predictions directly.

**Code**
```{r}
# plot predictions and confidence intervals
galton_heights %>% ggplot(aes(son, father)) +
  geom_point() +
  geom_smooth(method = "lm")

# predict Y directly
fit <- galton_heights %>% lm(son ~ father, data = .) 
Y_hat <- predict(fit, se.fit = TRUE)
names(Y_hat)

# plot best fit line
galton_heights %>%
  mutate(Y_hat = predict(lm(son ~ father, data=.))) %>%
  ggplot(aes(father, Y_hat))+
  geom_line()
```

### Assessment: Least Squares Estimates, part 1
#### Question 1
The following code was used in the video to plot RSS with $\beta_0=25$.

```{r}
beta1 = seq(0, 1, len=nrow(galton_heights))
results <- data.frame(beta1 = beta1,
                      rss = sapply(beta1, rss, beta0 = 25))
results %>% ggplot(aes(beta1, rss)) + geom_line() + 
  geom_line(aes(beta1, rss), col=2)
```

In a model for sons’ heights vs fathers’ heights, what is the least squares estimate (LSE) for $\beta_1$ if we assume $\hat{\beta}_0$ is 36?
Hint: modify the code above to do your analysis.

- 0.65
- **0.5**
- 0.2
- 12

**Answer**  
You can tell from a plot of RSS vs $\beta_1$ that the minimum estimate is 0.5

**Explanation**  
Using the code from the video, you can plot RSS vs  β1  to find the value for  β1  that minimizes the RSS. In this case, that value is 0.5 when we assume that  β^0  is 36.

When we assumed that $\hat{\beta}_0$ was 25, as in the sample code, the LSE for $\beta_1$ was 0.65.

#### Question 2
The least squares estimates for the parameters $\beta_0,\beta_1,…,\beta_n$ minimize the residual sum of squares.

**Explanation**  
The least squares estimates minimize, not maximize, the residual sum of squares.

#### Question 3
Load the `Lahman` library and filter the `Teams` data frame to the years 1961-2001. Run a linear model in R predicting the number of runs per game based on *both* the number of bases on balls per game *and* the number of home runs per game.

What is the coefficient for bases on balls?

- **0.39**
- 1.56
- 1.74
- 0.027

**Explanation**  
The coefficient for bases on balls is 0.39; the coefficient for home runs is 1.56; the intercept is 1.74; the standard error for the BB coefficient is 0.027.

#### Question 4
We run a Monte Carlo simulation where we repeatedly take samples of N = 100 from the Galton heights data and compute the regression slope coefficients for each sample:

```{r}
B <- 1000
N <- 100
lse <- replicate(B, {
    sample_n(galton_heights, N, replace = TRUE) %>% 
    lm(son ~ father, data = .) %>% .$coef 
})

lse <- data.frame(beta_0 = lse[1,], beta_1 = lse[2,]) 
```

What does the central limit theorem tell us about the variables beta_0 and beta_1?
Select ALL that apply.

- **They are approximately normally distributed.**
- **The expected value of each is the true value of** $\mathbf{\beta_0}$ **and** $\mathbf{\beta_1}$ **(assuming the Galton heights data is a complete population).**
- The central limit theorem does not apply in this situation.
- It allows us to test the hypothesis that  $\beta_0=0$ and $\beta_1=0$.

**Answer**  
With a large enough N, the distributions of both beta_0 and beta_1 are approximately normal.

**Explanation**  
With a large enough N, the central limit theorem applies and tells us that the distributions of both beta_0 and beta_1 are approximately normal. The expected values of beta_0 and beta_1 are the true values of $\beta_0$ and $\beta_1$, assuming that the Galton heights data are a complete population.

For hypothesis testing, we assume that the errors in the model are normally distributed.

#### Question 5
In an earlier video, we ran the following linear model and looked at a summary of the results.

```
> mod <- lm(son ~ father, data = galton_heights)
> summary(mod)

Call:
lm(formula = son ~ father, data = galton_heights)

Residuals:
   Min     1Q  Median     3Q    Max 
-5.902  -1.405  0.092    1.342  8.092 

Coefficients:
                 Estimate  Std. Error  t value     Pr(>|t|)  
(Intercept)     35.7125     4.5174       7.91    2.8e-13 ***  
father           0.5028     0.0653       7.70    9.5e-13 ***  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
```

What null hypothesis is the second p-value (the one in the father row) testing?
 
- $\beta_1=1$, where $\beta_1$ is the coefficient for the variable "father."
- $\beta_1=0.503$, where $\beta_1$ is the coefficient for the variable "father."
- $\mathbf{\beta_1=0}$, **where** $\mathbf{\beta_1}$ **is the coefficient for the variable "father.**" 

**Explanation**  
The p-value for "father" tests the null hypothesis that $\beta_1=0$, i.e., the fathers' heights are not associated with the sons' heights, where $\beta_1$ is the coefficient for the variable father.

#### Question 6
Which R code(s) below would properly plot the predictions and confidence intervals for our linear model of sons’ heights?
Select ALL that apply.

- 
```
galton_heights %>% ggplot(aes(father, son)) +
    geom_point() +
    geom_smooth()
```
- **`galton_heights %>% ggplot(aes(father, son)) +`**  
**`    geom_point() +`**  
**`    geom_smooth(method = "lm")`**  
- 
**`model <- lm(son ~ father, data = galton_heights)`**  
**`predictions <- predict(model, interval = c("confidence"), level = 0.95)`**  
**`data <- as.tibble(predictions) %>% bind_cols(father = galton_heights$father)`**  

**`ggplot(data, aes(x = father, y = fit)) +`**  
**`    geom_line(color = "blue", size = 1) + `**  
**`    geom_ribbon(aes(ymin=lwr, ymax=upr), alpha=0.2) + `**  
**`    geom_point(data = galton_heights, aes(x = father, y = son))`**  

- 
```
model <- lm(son ~ father, data = galton_heights)
predictions <- predict(model)
data <- as.tibble(predictions) %>% bind_cols(father = galton_heights$father)

ggplot(data, aes(x = father, y = fit)) +
    geom_line(color = "blue", size = 1) + 
    geom_point(data = galton_heights, aes(x = father, y = son))
```
**Answer**  
This is one way to plot predictions and confidence intervals for a linear model of sons’ heights vs. fathers’ heights. This is one of two correct answers.
Correct. This code uses the predict command to generate predictions and 95% confidence intervals for the linear model of sons’ heights vs. fathers’ heights. This is one of two correct answers.

### Assessment: Least Squares Estimates, part 2
In Questions 7 and 8, you'll look again at female heights from **GaltonFamilies**.

Define **female_heights**, a set of mother and daughter heights sampled from **GaltonFamilies**, as follows:

```{r}
set.seed(1989, sample.kind="Rounding") #if you are using R 3.6 or later
library(HistData)
data("GaltonFamilies")
options(digits = 3)    # report 3 significant digits
female_heights <- GaltonFamilies %>%     
    filter(gender == "female") %>%     
    group_by(family) %>%     
    sample_n(1) %>%     
    ungroup() %>%     
    select(mother, childHeight) %>%     
    rename(daughter = childHeight) 
```

#### Question 7
Fit a linear regression model predicting the mothers' heights using daughters' heights.

What is the slope of the model?
```{r}
fit <- lm(mother ~ daughter, data = female_heights)
fit$coef[2]
```

What the intercept of the model?
```{r}
fit$coef[1]
```

#### Question 8
Predict mothers' heights using the model.

What is the predicted height of the first mother in the dataset?
```{r}
predict(fit)[1]
```

What is the actual height of the first mother in the dataset?
```{r}
female_heights$mother[1]
```

We have shown how BB and singles have similar predictive power for scoring runs. Another way to compare the usefulness of these baseball metrics is by assessing how stable they are across the years. Because we have to pick players based on their previous performances, we will prefer metrics that are more stable. In these exercises, we will compare the stability of singles and BBs.

Before we get started, we want to generate two tables: one for 2002 and another for the average of 1999-2001 seasons. We want to define per plate appearance statistics, keeping only players with more than 100 plate appearances. Here is how we create the 2002 table:

```{r}
library(Lahman)
bat_02 <- Batting %>% filter(yearID == 2002) %>%
    mutate(pa = AB + BB, singles = (H - X2B - X3B - HR)/pa, bb = BB/pa) %>%
    filter(pa >= 100) %>%
    select(playerID, singles, bb)

```

#### Question 9
Now compute a similar table but with rates computed over 1999-2001. Keep only rows from 1999-2001 where players have 100 or more plate appearances, then calculate the average single rate (`mean_singles`) and average BB rate (`mean_bb`) per player over those three seasons.

How many players had a single rate `mean_singles` of greater than 0.2 per plate appearance over 1999-2001?
```{r}
bat_99_01 <- Batting %>% filter(yearID %in% 1999:2001) %>%
    mutate(pa = AB + BB, singles = (H - X2B - X3B - HR)/pa, bb = BB/pa) %>%
    filter(pa >= 100) %>%
    group_by(playerID) %>%
    summarize(mean_singles = mean(singles), mean_bb = mean(bb))
sum(bat_99_01$mean_singles > 0.2)
```

How many players had a BB rate mean_bb of greater than 0.2 per plate appearance over 1999-2001?
```{r}
sum(bat_99_01$mean_bb > 0.2)
```

#### Question 10
Use `inner_join` to combine the `bat_02` table with the table of 1999-2001 rate averages you created in the previous question.

What is the correlation between 2002 singles rates and 1999-2001 average singles rates?
```{r}
dat <- inner_join(bat_02, bat_99_01)
cor(dat$singles, dat$mean_singles)
```

What is the correlation between 2002 BB rates and 1999-2001 average BB rates?
```{r}
cor(dat$bb, dat$mean_bb)
```

#### Question 11
Make scatterplots of `mean_singles` versus `singles` and `mean_bb` versus `bb`.

Are either of these distributions bivariate normal?

- Neither distribution is bivariate normal.
- `singles` and `mean_singles` are bivariate normal, but `bb` and `mean_bb` are not.
- `bb` and `mean_bb` are bivariate normal, but `singles` and `mean_singles` are not.
- Both distributions are bivariate normal.

**Explanation**  
Both distributions are bivariate normal, as can be seen in the scatter plots made using the following code:
```{r}
dat %>%
    ggplot(aes(singles, mean_singles)) +
    geom_point()
dat %>%
    ggplot(aes(bb, mean_bb)) +
    geom_point()
```

#### Question 12
Fit a linear model to predict 2002 singles given 1999-2001 mean_singles.

What is the coefficient of mean_singles, the slope of the fit?
```{r}
fit_singles <- lm(singles ~ mean_singles, data = dat)
fit_singles$coef[2]
```

Fit a linear model to predict 2002 bb given 1999-2001 mean_bb.

What is the coefficient of `mean_bb`, the slope of the fit?
```{r}
fit_bb <- lm(bb ~ mean_bb, data = dat)
fit_bb$coef[2]
```

## 2.3: Tibbles, do, and broom
### Advanced dplyr: Tibbles
**Textbook link**  
The textbook section [discussing tibbles can be found here.](https://rafalab.github.io/dsbook/linear-models.html#linear-regression-in-the-tidyverse)

**Key points**

- Tibbles can be regarded as a modern version of data frames and are the default data structure in the tidyverse.
- Some functions that do not work properly with data frames do work with tibbles.

**Code**
```{r}
# stratify by HR
dat <- Teams %>% filter(yearID %in% 1961:2001) %>%
  mutate(HR = round(HR/G, 1), 
         BB = BB/G,
         R = R/G) %>%
  select(HR, BB, R) %>%
  filter(HR >= 0.4 & HR<=1.2)

# calculate slope of regression lines to predict runs by BB in different HR strata
dat %>%  
  group_by(HR) %>%
  summarize(slope = cor(BB,R)*sd(R)/sd(BB))

# use lm to get estimated slopes - lm does not work with grouped tibbles
dat %>%  
  group_by(HR) %>%
  lm(R ~ BB, data = .) %>%
  .$coef

# inspect a grouped tibble
dat %>% group_by(HR) %>% head()
dat %>% group_by(HR) %>% class()
```

### Tibbles: Differences from Data Frames
**Key points**

- Tibbles are more readable than data frames.
- If you subset a data frame, you may not get a data frame. If you subset a tibble, you always get a tibble.
- Tibbles can hold more complex objects such as lists or functions.
- Tibbles can be grouped.

**Code**
```{r}
# inspect data frame and tibble
Teams
as.tibble(Teams)

# subsetting a data frame sometimes generates vectors
class(Teams[,20])

# subsetting a tibble always generates tibbles
class(as.tibble(Teams[,20]))

# pulling a vector out of a tibble
class(as.tibble(Teams)$HR)

# access a non-existing column in a data frame or a tibble
Teams$hr
as.tibble(Teams)$hr

# create a tibble with complex objects
tibble(id = c(1, 2, 3), func = c(mean, median, sd))
```

### do
**Textbook link** 
The textbook section [discussing the **do** function can be found here.](https://rafalab.github.io/dsbook/linear-models.html#linear-regression-in-the-tidyverse)

**Key points**

- The **`do()`** function serves as a bridge between R functions, such as **`lm()`**, and the **tidyverse**.
- We have to specify a column when using the `do()` function, otherwise we will get an error.
- If the data frame being returned has more than one row, the rows will be concatenated appropriately.

**Code**
```{r}
# use do to fit a regression line to each HR stratum
dat %>%  
    group_by(HR) %>%
    do(fit = lm(R ~ BB, data = .))

```
```
# using do without a column name gives an error
dat %>%
    group_by(HR) %>%
    do(lm(R ~ BB, data = .))
Error: Results 1, 2, 3, 4, 5, ... must be data frames, not lm
```
```{r}
# define a function to extract slope from lm
get_slope <- function(data){
  fit <- lm(R ~ BB, data = data)
  data.frame(slope = fit$coefficients[2], 
             se = summary(fit)$coefficient[2,2])
}

# return the desired data frame
dat %>%  
  group_by(HR) %>%
  do(get_slope(.))

# not the desired output: a column containing data frames
dat %>%  
  group_by(HR) %>%
  do(slope = get_slope(.))


# data frames with multiple rows will be concatenated appropriately
get_lse <- function(data){
  fit <- lm(R ~ BB, data = data)
  data.frame(term = names(fit$coefficients),
    slope = fit$coefficients, 
    se = summary(fit)$coefficient[,2])
}
dat %>%  
  group_by(HR) %>%
  do(get_lse(.))
```

### broom
**Textbook link**  
The textbook section on the [**broom** package can be found here.](https://rafalab.github.io/dsbook/linear-models.html#the-broom-package)

**Key points**

- The **broom** package has three main functions, all of which extract information from the object returned by `lm` and return it in a **tidyverse** friendly data frame.
- The `tidy` function returns estimates and related information as a data frame.
- The functions `glance` and `augment` relate to model specific and observation specific outcomes respectively.

**Code**
```{r}
# use tidy to return lm estimates and related information as a data frame
library(broom)
fit <- lm(R ~ BB, data = dat)
tidy(fit)

# add confidence intervals with tidy
tidy(fit, conf.int = TRUE)

# pipeline with lm, do, tidy
dat %>%  
  group_by(HR) %>%
  do(tidy(lm(R ~ BB, data = .), conf.int = TRUE)) %>%
  filter(term == "BB") %>%
  select(HR, estimate, conf.low, conf.high)

# make ggplots
dat %>%  
  group_by(HR) %>%
  do(tidy(lm(R ~ BB, data = .), conf.int = TRUE)) %>%
  filter(term == "BB") %>%
  select(HR, estimate, conf.low, conf.high) %>%
  ggplot(aes(HR, y = estimate, ymin = conf.low, ymax = conf.high)) +
  geom_errorbar() +
  geom_point()

# inspect with glance
glance(fit)
```

### Assessment: Tibbles, do, and broom, part 1
#### Question 1
As seen in the videos, what problem do we encounter when we try to run a linear model on our baseball data, grouping by home runs?

- There is not enough data in some levels to run the model.
- **The `lm` function does not know how to handle grouped tibbles.**
- The results of the `lm` function cannot be put into a tidy format.

**Explanation**  
The `lm` function does not know how to handle grouped tibbles, so we can't simply run a linear model on the baseball data grouped by home runs. We need something to bridge between the grouped tibble and the `lm` function.

#### Question 2
 Tibbles are similar to what other class in R?
 
- Vectors
- Matrices
- **Data frames**
- Lists

**Explanation**  
Tibbles are essentially modern versions of data frames.

#### Question 3
What are some advantages of tibbles compared to data frames?
Select ALL that apply.

- **Tibbles display better.**  
- **If you subset a tibble, you always get back a tibble.**  
- **Tibbles can have complex entries.**  
- **Tibbles can be grouped.**  

**Explanation**  
All of the listed answers are advantages of tibbles when compared to data frames: tibbles display better, they always return tibbles when subsetted, they can have complex entries, and they can be grouped.

#### Question 4
What are two advantages of the `do` command, when applied to the tidyverse?
Select TWO.

- It is faster than normal functions.
- It returns useful error messages.
- **It understands grouped tibbles.**  
- **It always returns a data.frame.**  

**Answer**  
The `do` function can understand grouped tibbles.
The `do` function always returns a data.frame.

**Explanation**  
The `do` function serves as a useful bridge between base R functions and the tidyverse. It understands grouped tibbles and always returns a `data.frame`.

#### Question 5
You want to take the tibble `dat`, which we used in the video on the `do` function, and run the linear model R ~ BB for each strata of HR. Then you want to add three new columns to your grouped tibble: the coefficient, standard error, and p-value for the BB term in the model.

You’ve already written the function get_slope, shown below.

```{r}
get_slope <- function(data) {
  fit <- lm(R ~ BB, data = data)
  sum.fit <- summary(fit)

  data.frame(slope = sum.fit$coefficients[2, "Estimate"], 
             se = sum.fit$coefficients[2, "Std. Error"],
             pvalue = sum.fit$coefficients[2, "Pr(>|t|)"])
}
```

What additional code could you write to accomplish your goal?

- 
```
dat %>% 
  group_by(HR) %>% 
  do(get_slope)
```
- **`dat %>% `**  
**`  group_by(HR) %>% `**  
**`  do(get_slope(.))`**  

- 
```
dat %>% 
  group_by(HR) %>% 
  do(slope = get_slope(.))
```
- 
```
dat %>% 
  do(get_slope(.))
```

**Answer**  
This will create a tibble with four columns: `HR`, `slope`, `se`, and `pvalue` for each level of `HR`.

**Explanation**  
```{r}
dat %>% 
  group_by(HR) %>% 
  do(get_slope(.))
```
This is the only command that correctly creates a tibble with four columns: `HR`, `slope`, `se`, and `pvalue` for each level of HR. The data frame must be passed to `get_slope` using `.`. If you name the results of the `do` command such as in the code `do(slope = get_slope(.))`, that will save all results in a single column called `slope`. If you forget `group_by`, then the results will be a model on the data as a whole, rather than on the data stratified by home runs.

#### Question 6
The output of a `broom` function is always what?

- **A data.frame**
- A list
- A vector

**Explanation**  
The `broom` function always outputs a `data.frame`.

#### Question 7
You want to know whether the relationship between home runs and runs per game varies by baseball league. You create the following dataset:

```{r}
dat <- Teams %>% filter(yearID %in% 1961:2001) %>%
  mutate(HR = HR/G,
         R = R/G) %>%
  select(lgID, HR, BB, R) 
```

What code would help you quickly answer this question?

- **`dat %>% `**  
**`  group_by(lgID) %>% `**  
**`  do(tidy(lm(R ~ HR, data = .), conf.int = T)) %>% `**  
**`  filter(term == "HR") `**  
- 
```
dat %>% 
  group_by(lgID) %>% 
  do(glance(lm(R ~ HR, data = .)))
```
- 
```
dat %>% 
  do(tidy(lm(R ~ HR, data = .), conf.int = T)) %>% 
  filter(term == "HR")
```
- 
```
dat %>% 
  group_by(lgID) %>% 
  do(mod = lm(R ~ HR, data = .))
```

**Answer**  
This is a good application of the command `tidy`, from the `broom` package.

**Explanation**  
```{r}
dat %>% 
  group_by(lgID) %>% 
  do(tidy(lm(R ~ HR, data = .), conf.int = T)) %>% 
  filter(term == "HR") 
```
This code is a good application of the command `tidy`, from the `broom` package.

The `glance` function provides data on model fit rather than on effect estimates and confidence intervals. If you forget the line `group_by(lgID)`, your code will give you a single estimate for the entire dataset because you have not grouped the data by league ID.
```{r}
dat %>% 
  group_by(lgID) %>% 
  do(mod = lm(R ~ HR, data = .))
```

This code gives get a data.frame with the column `mod`, which contains the linear model results. While it is possible to then extract effect estimates and confidence intervals from this model, it is not nearly as easy as using the `tidy` function.

### Assessment: Tibbles, do, and broom, part 2
We have investigated the relationship between fathers' heights and sons' heights. But what about other parent-child relationships? Does one parent's height have a stronger association with child height? How does the child's gender affect this relationship in heights? Are any differences that we observe statistically significant?

The `galton` dataset is a sample of one male and one female child from each family in the **GaltonFamilies** dataset. The `pair` column denotes whether the pair is father and daughter, father and son, mother and daughter, or mother and son.

Create the galton dataset using the code below:
```{r}
library(tidyverse)
library(HistData)
data("GaltonFamilies")
set.seed(1) # if you are using R 3.5 or earlier
set.seed(1, sample.kind = "Rounding") # if you are using R 3.6 or later
galton <- GaltonFamilies %>%
    group_by(family, gender) %>%
    sample_n(1) %>%
    ungroup() %>% 
    gather(parent, parentHeight, father:mother) %>%
    mutate(child = ifelse(gender == "female", "daughter", "son")) %>%
    unite(pair, c("parent", "child"))

galton
```

#### Question 8
Group by `pair` and summarize the number of observations in each group.

How many father-daughter pairs are in the dataset?

How many mother-son pairs are in the dataset?

**Explanation**  
The following code will give the number of observations in each of the four groups:
```{r}
galton %>%
    group_by(pair) %>%
    summarize(n = n())
```

#### Question 9
Calculate the correlation coefficients for fathers and daughters, fathers and sons, mothers and daughters and mothers and sons.

Which pair has the **strongest** correlation in heights?

- fathers and daughters
- **fathers and sons**
- mothers and daughters
- mothers and sons

**Explanation**  
The following code will give the maximum correlation:
```{r}
galton %>%
    group_by(pair) %>%
    summarize(cor = cor(parentHeight, childHeight)) %>%
    filter(cor == max(cor))
```

Which pair has the **weakest** correlation in heights?

- fathers and daughters
- fathers and sons
- mothers and daughters
- **mothers and sons**

**Explanation**  
The following code will give the minimum correlation:
```{r}
galton %>%
    group_by(pair) %>%
    summarize(cor = cor(parentHeight, childHeight)) %>%
    filter(cor == min(cor))
```

#### Question 10
Question 10 has two parts. The information here applies to both parts.

Use `lm` and the `broom` package to fit regression lines for each parent-child pair type. Compute the least squares estimates, standard errors, confidence intervals and p-values for the `parentHeight` coefficient for each pair.

#### Question 10a
What is the estimate of the father-daughter coefficient?
```{r}
library(broom)
galton %>%
    group_by(pair) %>%
    do(tidy(lm(childHeight ~ parentHeight, data = .), conf.int = TRUE)) %>%
    filter(term == "parentHeight", pair == "father_daughter") %>%
    pull(estimate)
```

For every 1-inch increase in mother's height, how many inches does the typical son's height increase?
Give your answer as a number with no units.
```{r}
galton %>%
    group_by(pair) %>%
    do(tidy(lm(childHeight ~ parentHeight, data = .), conf.int = TRUE)) %>%
    filter(term == "parentHeight", pair == "mother_son") %>%
    pull(estimate)
```

#### Question 10b
Which sets of parent-child heights are significantly correlated at a p-value cut off of .05?
Select ALL that apply.

- **father-daughter**  
- **father-son**  
- **mother-daughter**  
- **mother-son**  

**Explanation**  
All of the parent-child heights are correlated with a p-value of <0.05.

Which of the following statements are true?
Select ALL that apply.

- **All of the confidence intervals overlap each other.**
- At least one confidence interval covers zero.
- **The confidence intervals involving mothers' heights are larger than the confidence intervals involving fathers' heights.**
- The confidence intervals involving daughters' heights are larger than the confidence intervals involving sons' heights.
- **The data are consistent with inheritance of height being independent of the child's gender.**
- **The data are consistent with inheritance of height being independent of the parent's gender.**

**Explanation**  
The following code can be used to answer both questions:
```{r}
galton %>%
    group_by(pair) %>%
    do(tidy(lm(childHeight ~ parentHeight, data = .), conf.int = TRUE)) %>%
    filter(term == "parentHeight" & p.value < .05)
```

All four of the confidence intervals overlap. The confidence intervals for mothers' heights are larger than those for fathers' heights, as observed from the standard errors. Because the confidence intervals overlap, the data are consistent with inheritance of height being independent of the child's or the parent's gender.

## 2.4: Regression and Baseball
### Building a Better Offensive Metric for Baseball
**Textbook link**  
The textbook section on the [continuation of the Moneyball case study can be found here.](https://rafalab.github.io/dsbook/linear-models.html#case-study-moneyball-continued)

**Code**
```{r}
# linear regression with two variables
fit <- Teams %>% 
  filter(yearID %in% 1961:2001) %>% 
  mutate(BB = BB/G, HR = HR/G,  R = R/G) %>%  
  lm(R ~ BB + HR, data = .)
tidy(fit, conf.int = TRUE)

# regression with BB, singles, doubles, triples, HR
fit <- Teams %>% 
  filter(yearID %in% 1961:2001) %>% 
  mutate(BB = BB / G, 
         singles = (H - X2B - X3B - HR) / G, 
         doubles = X2B / G, 
         triples = X3B / G, 
         HR = HR / G,
         R = R / G) %>%  
  lm(R ~ BB + singles + doubles + triples + HR, data = .)
coefs <- tidy(fit, conf.int = TRUE)
coefs

# predict number of runs for each team in 2002 and plot
Teams %>% 
  filter(yearID %in% 2002) %>% 
  mutate(BB = BB/G, 
         singles = (H-X2B-X3B-HR)/G, 
         doubles = X2B/G, 
         triples =X3B/G, 
         HR=HR/G,
         R=R/G)  %>% 
  mutate(R_hat = predict(fit, newdata = .)) %>%
  ggplot(aes(R_hat, R, label = teamID)) + 
  geom_point() +
  geom_text(nudge_x=0.1, cex = 2) + 
  geom_abline()

# average number of team plate appearances per game
pa_per_game <- Batting %>% filter(yearID == 2002) %>% 
  group_by(teamID) %>%
  summarize(pa_per_game = sum(AB+BB)/max(G)) %>% 
  pull(pa_per_game) %>% 
  mean

# compute per-plate-appearance rates for players available in 2002 using previous data
players <- Batting %>% filter(yearID %in% 1999:2001) %>% 
  group_by(playerID) %>%
  mutate(PA = BB + AB) %>%
  summarize(G = sum(PA)/pa_per_game,
    BB = sum(BB)/G,
    singles = sum(H-X2B-X3B-HR)/G,
    doubles = sum(X2B)/G, 
    triples = sum(X3B)/G, 
    HR = sum(HR)/G,
    AVG = sum(H)/sum(AB),
    PA = sum(PA)) %>%
  filter(PA >= 300) %>%
  select(-G) %>%
  mutate(R_hat = predict(fit, newdata = .))

# plot player-specific predicted runs
qplot(R_hat, data = players, geom = "histogram", binwidth = 0.5, color = I("black"))

# add 2002 salary of each player
players <- Salaries %>% 
  filter(yearID == 2002) %>%
  select(playerID, salary) %>%
  right_join(players, by="playerID")

# add defensive position
position_names <- c("G_p","G_c","G_1b","G_2b","G_3b","G_ss","G_lf","G_cf","G_rf")
tmp_tab <- Appearances %>% 
  filter(yearID == 2002) %>% 
  group_by(playerID) %>%
  summarize_at(position_names, sum) %>%
  ungroup()  
pos <- tmp_tab %>%
  select(position_names) %>%
  apply(., 1, which.max) 
players <- data_frame(playerID = tmp_tab$playerID, POS = position_names[pos]) %>%
  mutate(POS = str_to_upper(str_remove(POS, "G_"))) %>%
  filter(POS != "P") %>%
  right_join(players, by="playerID") %>%
  filter(!is.na(POS)  & !is.na(salary))

# add players' first and last names
players <- Master %>%
  select(playerID, nameFirst, nameLast, debut) %>%
  mutate(debut = as.Date(debut)) %>%
  right_join(players, by="playerID")

# top 10 players
players %>% select(nameFirst, nameLast, POS, salary, R_hat) %>% 
  arrange(desc(R_hat)) %>% 
  top_n(10) 

# players with a higher metric have higher salaries
players %>% ggplot(aes(salary, R_hat, color = POS)) + 
  geom_point() +
  scale_x_log10()

# remake plot without players that debuted after 1998
library(lubridate)
players %>% filter(year(debut) < 1998) %>%
 ggplot(aes(salary, R_hat, color = POS)) + 
  geom_point() +
  scale_x_log10()
```

### Building a Better Offensive Metric for Baseball: Linear Programming
A way to actually pick the players for the team can be done using what computer scientists call **linear programming**. Although we don't go into this topic in detail in this course, we include the code anyway:
```{r}
library(reshape2)
library(lpSolve)

players <- players %>% filter(debut <= "1997-01-01" & debut > "1988-01-01")
constraint_matrix <- acast(players, POS ~ playerID, fun.aggregate = length)
npos <- nrow(constraint_matrix)
constraint_matrix <- rbind(constraint_matrix, salary = players$salary)
constraint_dir <- c(rep("==", npos), "<=")
constraint_limit <- c(rep(1, npos), 50*10^6)
lp_solution <- lp("max", players$R_hat,
                  constraint_matrix, constraint_dir, constraint_limit,
                  all.int = TRUE) 
```

This algorithm chooses these 9 players:
```{r}
our_team <- players %>%
  filter(lp_solution$solution == 1) %>%
  arrange(desc(R_hat))
our_team %>% select(nameFirst, nameLast, POS, salary, R_hat)
```
```
  nameFirst    nameLast POS   salary R_hat
1     Jason      Giambi  1B 10428571  7.99
2     Nomar Garciaparra  SS  9000000  7.51
3      Mike      Piazza   C 10571429  7.16
4      Phil       Nevin  3B  2600000  6.75
5      Jeff        Kent  2B  6000000  6.68
```
We note that these players all have above average BB and HR rates while the same is not true for singles.
```{r}
my_scale <- function(x) (x - median(x))/mad(x)
players %>% mutate(BB = my_scale(BB), 
                   singles = my_scale(singles),
                   doubles = my_scale(doubles),
                   triples = my_scale(triples),
                   HR = my_scale(HR),
                   AVG = my_scale(AVG),
                   R_hat = my_scale(R_hat)) %>%
    filter(playerID %in% our_team$playerID) %>%
    select(nameFirst, nameLast, BB, singles, doubles, triples, HR, AVG, R_hat) %>%
    arrange(desc(R_hat))
```
```
  nameFirst    nameLast    BB singles doubles triples    HR  AVG R_hat
1     Jason      Giambi 3.317 -0.5315   0.754  -0.675 2.067 2.63  3.54
2     Nomar Garciaparra 0.284  1.7330   2.651   0.471 1.003 3.95  2.97
3      Mike      Piazza 0.596 -0.0499  -0.177  -1.335 2.682 1.70  2.56
4      Phil       Nevin 0.790 -0.6751   0.670  -1.137 2.103 1.09  2.07
5      Jeff        Kent 0.875 -0.2717   1.833   1.210 0.967 1.66  2.00

```

### On Base Plus Slugging (OPS)
**Key points**  

The on-base-percentage plus slugging percentage (OPS) metric is:

\[\frac{BB}{PA}+\left (\frac{Singles+2Doubles+3Triples+4HR}{AB}\right )\]

### Regression Fallacy
**Textbook link**  
The textbook section on the [regression fallacy can be found here.](https://rafalab.github.io/dsbook/linear-models.html#the-regression-fallacy)

**Key points**  

- Regression can bring about errors in reasoning, especially when interpreting individual observations.
- The example showed in the video demonstrates that the **"sophomore slump"** observed in the data is caused by regressing to the mean.

**Code**  
The code to create a table with player ID, their names, and their most played position:
```{r}
library(Lahman)
playerInfo <- Fielding %>%
    group_by(playerID) %>%
    arrange(desc(G)) %>%
    slice(1) %>%
    ungroup %>%
    left_join(Master, by="playerID") %>%
    select(playerID, nameFirst, nameLast, POS)
```

The code to create a table with only the ROY award winners and add their batting statistics:
```{r}
ROY <- AwardsPlayers %>%
    filter(awardID == "Rookie of the Year") %>%
    left_join(playerInfo, by="playerID") %>%
    rename(rookie_year = yearID) %>%
    right_join(Batting, by="playerID") %>%
    mutate(AVG = H/AB) %>%
    filter(POS != "P")
```

The code to keep only the rookie and sophomore seasons and remove players who did not play sophomore seasons:
```{r}
ROY <- ROY %>%
    filter(yearID == rookie_year | yearID == rookie_year+1) %>%
    group_by(playerID) %>%
    mutate(rookie = ifelse(yearID == min(yearID), "rookie", "sophomore")) %>%
    filter(n() == 2) %>%
    ungroup %>%
    select(playerID, rookie_year, rookie, nameFirst, nameLast, AVG) 
```

The code to use the `spread` function to have one column for the rookie and sophomore years batting averages:
```{r}
ROY <- ROY %>% spread(rookie, AVG) %>% arrange(desc(rookie))

ROY
```

The code to calculate the proportion of players who have a lower batting average their sophomore year:
```{r}
mean(ROY$sophomore - ROY$rookie <= 0)
```
```
#> [1] 0.677
```

The code to do the similar analysis on all players that played the 2013 and 2014 seasons and batted more than 130 times (minimum to win Rookie of the Year):
```{r}
two_years <- Batting %>%
    filter(yearID %in% 2013:2014) %>%
    group_by(playerID, yearID) %>%
    filter(sum(AB) >= 130) %>%
    summarize(AVG = sum(H)/sum(AB)) %>%
    ungroup %>%
    spread(yearID, AVG) %>%
    filter(!is.na(`2013`) & !is.na(`2014`)) %>%
    left_join(playerInfo, by="playerID") %>%
    filter(POS!="P") %>%
    select(-POS) %>%
    arrange(desc(`2013`)) %>%
    select(nameFirst, nameLast, `2013`, `2014`)

two_years
```

The code to see what happens to the worst performers of 2013:
```{r}
arrange(two_years, `2013`)
```

The code to see  the correlation for performance in two separate years:
```{r}
qplot(`2013`, `2014`, data = two_years)

summarize(two_years, cor(`2013`,`2014`))
```

### Measurement Error Models
**Textbook link**  
The textbook section on [measurement error models can be found here.](https://rafalab.github.io/dsbook/linear-models.html#measurement-error-models)

**Key points**

- Up to now, all our linear regression examples have been applied to two or more random variables. We assume the pairs are bivariate normal and use this to motivate a linear model.
- Another use for linear regression is with **measurement error models**, where it is common to have a non-random covariate (such as time). Randomness is introduced from measurement error rather than sampling or natural variability.

**Code**  
The code to use `dslabs` function `rfalling_object` to generate simulations of dropping balls:
```{r}
library(dslabs)
falling_object <- rfalling_object()
```

The code to draw the trajectory of the ball:
```{r}
falling_object %>%
    ggplot(aes(time, observed_distance)) +
    geom_point() +
    ylab("Distance in meters") +
    xlab("Time in seconds")
```

The code to use the `lm()` function to estimate the coefficients:
```{r}
fit <- falling_object %>%
    mutate(time_sq = time^2) %>%
    lm(observed_distance~time+time_sq, data=.)

tidy(fit)
```
```
#> # A tibble: 3 x 5
#>   term        estimate std.error statistic  p.value
#>   <chr>          <dbl>     <dbl>     <dbl>    <dbl>
#> 1 (Intercept)    56.9      0.580     98.0  1.56e-17
#> 2 time           -1.04     0.829     -1.25 2.36e- 1
#> 3 time_sq        -4.73     0.246    -19.2  8.17e-10
```

The code to check if the estimated parabola fits the data:
```{r}
augment(fit) %>%
    ggplot() +
    geom_point(aes(time, observed_distance)) +
    geom_line(aes(time, .fitted), col = "blue")
```

The code to see the summary statistic of the regression:
```{r}
tidy(fit, conf.int = TRUE)
```
```
#> # A tibble: 3 x 7
#>   term        estimate std.error statistic  p.value conf.low conf.high
#>   <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>
#> 1 (Intercept)    56.9      0.580     98.0  1.56e-17    55.6     58.2  
#> 2 time           -1.04     0.829     -1.25 2.36e- 1    -2.86     0.784
#> 3 time_sq        -4.73     0.246    -19.2  8.17e-10    -5.27    -4.19
```


### Assessment: Regression and Baseball, part 1
#### Question 1
What is the final linear model (in the video "Building a Better Offensive Metric for Baseball") we used to predict runs scored per game?

- `lm(R ~ BB + HR)`
- `lm(HR ~ BB + singles + doubles + triples)`
- **`lm(R ~ BB + singles + doubles + triples + HR)`**
- `lm(R ~ singles + doubles + triples + HR)`

**Explanation**  
`lm(R ~ BB + singles + doubles + triples + HR)` is the only one of the models above that predicts runs scored based on all of the following: BBs, singles, doubles, triples, and HRs.

#### Question 2
We want to estimate runs per game scored by individual players, not just by teams. What summary metric do we calculate to help estimate this?

Look at the code from the video "Building a Metter Offensive Metric for Baseball" for a hint:
```{r}
pa_per_game <- Batting %>% 
  filter(yearID == 2002) %>% 
  group_by(teamID) %>%
  summarize(pa_per_game = sum(AB+BB)/max(G)) %>% 
  .$pa_per_game %>% 
  mean
```

The summary metric used is:

- `pa_per_game`: the mean number of plate appearances per team per game for each team
- `pa_per_game`: the mean number of plate appearances per game for each player
- ``pa_per_game`: **the number of plate appearances per team per game, averaged across all teams**

**Explanation**  
`pa_per_game` is the number of plate appearances per team per game averaged across all teams. We initially calculated the `pa_per_game` grouped by teams but then took the means across all teams to get one summary metric.

#### Question 3
Imagine you have two teams. Team A is comprised of batters who, on average, get two bases on balls, four singles, one double, and one home run. Team B is comprised of batters who, on average, get one base on balls, six singles, two doubles, and one triple.

Which team scores more runs, as predicted by our model?

- Team A
- **Team B**
- Tie
- Impossible to know

**Explanation**  
By using the coefficients from the linear model to predict the number of runs scored by each team, you find that Team B is expected to score more runs on average.

#### Question 4
The on-base-percentage plus slugging percentage (OPS) metric gives the most weight to:

- Singles
- Doubles
- Triples
- **Home Runs**

**Explanation**  
By looking at the equation for OPS, you can tell that the OPS metric weights home runs most heavily.

\[OPS=\frac{BB}{PA}+\frac{Singles+2Doubles+3Triples+4HR}{AB}\]

#### Question 5
What statistical concept properly explains the “sophomore slump”?

- **Regression to the mean**
- Law of averages
- Normal distribution

**Explanation**
Regression to the mean is what explains the sophomore slump. The correlation for performance in two separate years is high but not perfect, so high performers will tend to perform slightly worse in the following year (and low performers will tend to perform slightly better in the following year).

#### Question 6
In our model of time vs. observed_distance in the video "Measurement Error Models", the randomness of our data was due to:

- sampling
- natural variability
- **measurement error**

**Explanation**  
Measurement error models look at applications where randomness is introduced from measurement error instead of sampling or natural variability.

#### Question 7
Which of the following are important assumptions about the measurement errors in the experiment presented in the video "Measurement Error Models"?
Select ALL that apply.

- **The measurement error is random**
- **The measurement error is independent**
- **The measurement error has the same distribution for each time $i$**

**Explanation**  
In this model, we asumed that the measurement errors were random, independent from each other, and had the same distribution for each time $i$.

We also assumed that there was no bias, which means that $E[\epsilon]=0$.

#### Question 8
Which of the following scenarios would violate an assumption of our measurement error model?

- The experiment was conducted on the moon.
- **There was one position where it was particularly difficult to see the dropped ball.**
- The experiment was only repeated 10 times, not 100 times.
incorreta

**Explanation**  
If there were one position where it was particularly difficult to see the dropped ball, that would violate the assumption of randomness. If the experiment were conducted on the moon, that would simply predict a different gravitational constant. Repeating the experiment 10 instead of 100 times would not matter because we do not need a large sample for our assumptions to be valid in this model.

### Assessment: Regression and baseball, part 2
Question 9 has two parts. Use the information below to answer both parts.

Use the `Teams` data frame from the **Lahman** package. Fit a multivariate linear regression model to obtain the effects of BB and HR on Runs (`R`) in 1971. Use the `tidy` function in the `broom` package to obtain the results in a data frame.

*Clarification: For questions 9-11, do __not__ normalize by number of games. For example, use R rather than R/G.*

#### Question 9a
What is the estimate for the effect of BB on runs?
```{r}
library(Lahman)
library(broom)
Teams %>%
    filter(yearID == 1971) %>%
    lm(R ~ BB + HR, data = .) %>%
    tidy() %>%
    filter(term == "BB") %>%
    pull(estimate)
```

What is the estimate for the effect of HR on runs?
```{r}
Teams %>%
    filter(yearID == 1971) %>%
    lm(R ~ BB + HR, data = .) %>%
    tidy() %>%
    filter(term == "HR") %>%
    pull(estimate)
```
#### Question 9b
Interpret the p-values for the estimates using a cutoff of 0.05.

Which of the following is the correct interpretation?

- Both BB and HR have a nonzero effect on runs.
- **HR has a significant effect on runs, but the evidence is not strong enough to suggest BB also does.**
- BB has a significant effect on runs, but the evidence is not strong enough to suggest HR also does.
- Neither BB nor HR have a statistically significant effect on runs.

**Explanation**  
The p-value for HR is less than 0.05, but the p-value of BB is greater than 0.05 (0.06), so the evidence is not strong enough to suggest that BB has a significant effect on runs at a p-value cutoff of 0.05.

#### Question 10
Repeat the above exercise to find the effects of BB and HR on runs (`R`) for every year from 1961 to 2018 using `do` and the `broom` package.

Make a scatterplot of the estimate for the effect of BB on runs over time and add a trend line with confidence intervals.

Fill in the blank to complete the statement:

The effect of BB on runs has **increased** over time.

**Explanation**  
The scatterplot with trendline can be made using the following code:

```{r}
res <- Teams %>%
    filter(yearID %in% 1961:2018) %>%
    group_by(yearID) %>%
    do(tidy(lm(R ~ BB + HR, data = .))) %>%
    ungroup() 
res %>%
    filter(term == "BB") %>%
    ggplot(aes(yearID, estimate)) +
    geom_point() +
    geom_smooth(method = "lm")
```

#### Question 11
Fit a linear model on the results from Question 10 to determine the effect of year on the impact of BB.

For each additional year, by what value does the impact of BB on runs change?  
Recall from the instructions at the top of the page that you should not normalize by the number of games (use `R` and `BB` instead of `R/G` and `BB/G`.
```{r}
res %>%
    filter(term == "BB") %>%
    lm(estimate ~ yearID, data = .) %>%
    tidy() %>%
    filter(term == "yearID") %>%
    pull(estimate)
```

What is the p-value for this effect?
```{r}
res %>%
    filter(term == "BB") %>%
    lm(estimate ~ yearID, data = .) %>%
    tidy() %>%
    filter(term == "yearID") %>%
    pull(p.value)
```

# Section 3: Confounding
## Confounding Overview
In the **Confounding** section, you will learn what is perhaps the most important lesson of statistics: that correlation is not causation.

After completing this section, you will be able to:

- Identify examples of **spurious correlation** and explain how **data dredging** can lead to spurious correlation.
- Explain how **outliers** can drive correlation and learn to adjust for outliers using **Spearman correlation**.
- Explain how **reversing cause and effect** can lead to associations being confused with causation.
- Understand how **confounders** can lead to the misinterpretation of associations.
- Explain and give examples of **Simpson's Paradox**.

This section has one part: **Correlation is Not Causation**. There is a comprehension checks at the end of this part, along with an assessment at the end of the section for Verified learners only.

We encourage you to use R to interactively test out your answers and further your own learning. If you get stuck, we encourage you to search the discussion boards for the answer to your issue or ask us for help!

## Correlation is Not Causation
### Correlation is Not Causation: Spurious Correlation
**Textbook link**  
The textbook section on [spurious correlation can be found here.](https://rafalab.github.io/dsbook/association-is-not-causation.html#spurious-correlation)

**Key points**

- Association/correlation is not causation.
- p-hacking is a topic of much discussion because it is a problem in scientific publications. Because publishers tend to reward statistically significant results over negative results, there is an incentive to report significant results.

**Code**
```{r}
# generate the Monte Carlo simulation
N <- 25
g <- 1000000
sim_data <- tibble(group = rep(1:g, each = N), x = rnorm(N * g), y = rnorm(N * g))

# calculate correlation between X,Y for each group
res <- sim_data %>% 
  group_by(group) %>% 
  summarize(r = cor(x, y)) %>% 
  arrange(desc(r))
res

# plot points from the group with maximum correlation
sim_data %>% filter(group == res$group[which.max(res$r)]) %>%
  ggplot(aes(x, y)) +
  geom_point() + 
  geom_smooth(method = "lm")

# histogram of correlation in Monte Carlo simulations
res %>% ggplot(aes(x=r)) + geom_histogram(binwidth = 0.1, color = "black")

# linear regression on group with maximum correlation
library(broom)
sim_data %>% 
  filter(group == res$group[which.max(res$r)]) %>%
  do(tidy(lm(y ~ x, data = .)))
```

### Correlation is Not Causation: Outliers
**Textbook link**  
The textbook section on [outliers can be found here.](https://rafalab.github.io/dsbook/association-is-not-causation.html#outliers-1)

**Key points**

- Correlations can be caused by **outliers**.
- The **Spearman correlation** is calculated based on the ranks of data.

**Code**
```{r}
# simulate independent X, Y and standardize all except entry 23
set.seed(1985)
x <- rnorm(100,100,1)
y <- rnorm(100,84,1)
x[-23] <- scale(x[-23])
y[-23] <- scale(y[-23])

# plot shows the outlier
qplot(x, y, alpha = 0.5)

# outlier makes it appear there is correlation
cor(x,y)
cor(x[-23], y[-23])

# use rank instead
qplot(rank(x), rank(y))
cor(rank(x), rank(y))

# Spearman correlation with cor function
cor(x, y, method = "spearman")
```

### Correlation is Not Causation: Reversing Cause and Effect
**Textbook link**  
The textbook section on [reversing cause and effect can be found here.](https://rafalab.github.io/dsbook/association-is-not-causation.html#reversing-cause-and-effect)

**Key points**

- Another way association can be confused with causation is when the **cause and effect are reversed.**
- As discussed in the video, in the Galton data, when father and son were reversed in the regression, the model was technically correct. The estimates and p-values were obtained correctly as well. What was incorrect was the **interpretation** of the model.

**Code**
```{r}
# cause and effect reversal using son heights to predict father heights
library(HistData)
data("GaltonFamilies")
GaltonFamilies %>%
  filter(childNum == 1 & gender == "male") %>%
  select(father, childHeight) %>%
  rename(son = childHeight) %>% 
  do(tidy(lm(father ~ son, data = .)))
```

### Correlation is Not Causation: Confounders
**Textbook link**  
The textbook section on [confounders can be found here.](https://rafalab.github.io/dsbook/association-is-not-causation.html#confounders)

**Key points**

- If X and Y are correlated, we call Z a **confounder** if changes in Z causes changes in both X and Y.

**Code**
```{r}
# UC-Berkeley admission data
library(dslabs)
data(admissions)
admissions

# percent men and women accepted
admissions %>% group_by(gender) %>% 
  summarize(percentage = 
              round(sum(admitted*applicants)/sum(applicants),1))

# test whether gender and admission are independent
admissions %>% group_by(gender) %>% 
  summarize(total_admitted = round(sum(admitted / 100 * applicants)), 
            not_admitted = sum(applicants) - sum(total_admitted)) %>%
  select(-gender) %>% 
  do(tidy(chisq.test(.)))

# percent admissions by major
admissions %>% select(major, gender, admitted) %>%
  spread(gender, admitted) %>%
  mutate(women_minus_men = women - men)

# plot total percent admitted to major versus percent women applicants
admissions %>% 
  group_by(major) %>% 
  summarize(major_selectivity = sum(admitted * applicants) / sum(applicants),
            percent_women_applicants = sum(applicants * (gender=="women")) /
                                             sum(applicants) * 100) %>%
  ggplot(aes(major_selectivity, percent_women_applicants, label = major)) +
  geom_text()

# plot number of applicants admitted and not
admissions %>%
  mutate(yes = round(admitted/100*applicants), no = applicants - yes) %>%
  select(-applicants, -admitted) %>%
  gather(admission, number_of_students, -c("major", "gender")) %>%
  ggplot(aes(gender, number_of_students, fill = admission)) +
  geom_bar(stat = "identity", position = "stack") +
  facet_wrap(. ~ major)

admissions %>% 
  mutate(percent_admitted = admitted * applicants/sum(applicants)) %>%
  ggplot(aes(gender, y = percent_admitted, fill = major)) +
  geom_bar(stat = "identity", position = "stack")
# condition on major and then look at differences
admissions %>% ggplot(aes(major, admitted, col = gender, size = applicants)) + geom_point()
# average difference by major
admissions %>%  group_by(gender) %>% summarize(average = mean(admitted))
```

### Simpson's Paradox
**Textbook link**  
The textbook section on [Simpson's paradox can be found here.](https://rafalab.github.io/dsbook/association-is-not-causation.html#simpsons-paradox)

**Key points**

- Simpson’s Paradox happens when we see the sign of the correlation flip when comparing the entire dataset with specific strata. 

### Assessment: Correlation is Not Causation
#### Question 1
In the videos, we ran one million tests of correlation for two random variables, X and Y.

How many of these correlations would you expect to have a significant p-value ($p≤0.05$), just by chance?

- 5,000
- **50,000**
- 100,000
- It’s impossible to know

**Answer**  
In this example, the chance of finding a correlation when none exists is 0.05*1,000,000 chances.

**Explanation**  
The p-value is defined as the probability of finding the observed result when the null hypothesis (no correlation) is true. When we have a p-value of 0.05, this means the chance of finding a correlation when none exists is 5% - e.g., 0.05*1,000,000 chances, which is 50,000.

#### Question 2
Which of the following are examples of p-hacking?  
Select ALL that apply.

- **Looking for associations between an outcome and several exposures and only reporting the one that is significant.**
- **Trying several different models and selecting the one that yields the smallest p-value.**
- **Repeating an experiment multiple times and only reporting the one with the smallest p-value.**
- Using a Monte Carlo simulations in an analysis.

**Explanation**  
Repeating an experiment multiple times and only reporting the one with the smallest p-value, looking for associations between an outcome and several exposures and only reporting the one that is significant, and trying several different models and selecting the one that yields the smallest p-value are all examples of p-hacking.

Monte Carlo simulations do not necessarily lead to multiple testing problems such as p-hacking in and of themselves.

#### Question 3
The Spearman correlation coefficient is robust to outliers because:

- It drops outliers before calculating correlation.
- It is the correlation of standardized values.
- **It calculates correlation between ranks, not values.**

**Explanation**  
Because the Spearman correlation coefficient uses ranks instead of values to calculate correlation, it is more robust to outliers.

#### Question 4
Which of the following may be examples of reversed cause and effect?  
Select ALL that apply.

- **Past smokers who have quit smoking may be more likely to die from lung cancer.**
- Tall fathers are more likely to have tall sons.
- **People with high blood pressure tend to have a healthier diet.**
- **Individuals in a low social status have a higher risk of schizophrenia.**

**Answer**  
Correct - it is possible that a smoker would quit after being diagnosed, but is at a higher risk of dying. It does not mean that quitting smoking increases your risk of dying from lung cancer.
Correct. This could be reverse causation. Individuals with high blood pressure may have a healthier diet because they are trying to improve their blood pressure, not because a healthy diet causes high blood pressure.
Correct. This could be reverse causation. It is possible that individuals with schizophrenia are more likely to have a lower social status due to the challenges of managing their disease.

**Explanation**  
"Past smokers who have quit smoking may be more likely to die from lung cancer" is an example because it is possible that a smoker would quit after being diagnosed, but is at a higher risk of dying. However, it does not mean that quitting smoking increases your risk of dying from lung cancer.

For the statement "tall fathers are more likely to have tall sons," we know the order of causation because the fathers came before the sons, so this is not a case of reversed cause and effect.

"People with high blood pressure tend to have a healthier diet" could be reverse causation. Individuals with high blood pressure may have a healthier diet because they are trying to improve their blood pressure, not because a healthy diet causes high blood pressure.

"Individuals in a low social status have a higher risk of schizophrenia" could be reverse causation. Individuals with schizophrenia may be more likely to have a lower social status due to the challenges of managing their disease.

#### Question 5
What can you do to determine if you are misinterpreting results because of a confounder?

- Nothing. If the p-value says the result is significant, then it is.
- **More closely examine the results by stratifying and plotting the data.**
- Always assume that you are misinterpreting the results.
- Use linear models to tease out a confounder.

**Answer**  
Although you can sometimes use linear models, you can't always and exploratory data analysis (stratifying and plotting data) will help determine if there is a confounder.

**Explanation**  
Exploratory data analysis (stratifying and plotting data) can help determine if there is a confounder. Linear models cannot be used in all situations.

#### Question 6
Look again at the admissions data presented in the confounders video using `?admissions`.

What important characteristic of the table variables do you need to know to understand the calculations used in this video?

- The data are from 1973.
- The columns `major` and `gender` are of class character, while `admitted` and `applicants` are numeric.
- The data are from the `dslabs` package.
- **The column `admitted` is the percent of students admitted, while the column `applicants` is the total number of applicants.**

**Answer**  
In all data science projects, it is important to understand the data that you are working with.

**Explanation**  
Several of these statements are true but not relevant to understanding the calculations in the video. The only statement that is critical for the analysis is that "The column admitted is the percent of students admitted, while the column applicants is the total number of applicants." In all data science projects, it is important to understand the data that you are working with.

#### Question 7
In the example in the confounders video, major selectivity confounds the relationship between UC Berkley admission rates and gender because:

- It was harder for women to be admitted to UC Berkeley.
- **Major selectivity is associated with both admission rates and with gender, as women tended to apply to more selective majors.**
- Some majors are more selective than others.
- Major selectivity is not a confounder.

**Explanation**  
Major selectivity is a confounder because it is associated with both admission rate and with gender.

#### Question 8
Admission rates at UC Berkeley are an example of Simpson’s Paradox because:

- **It appears that men have higher a higher admission rate than women, however, after we stratify by major, we see that on average women have a higher admission rate than men.**
- It was a paradox that women were being admitted at a lower rate than men.
- The relationship between admissions and gender is confounded by major selectivity.

**Answer**  
This is a good explanation of why this example is considered an example of Simpson’s Paradox

**Explanation**  
Simpson’s Paradox refers specifically to cases where the sign of the correlation flips when comparing the entire dataset vs. specific strata, so only the first statement is correct.


